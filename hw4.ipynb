{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MDP Transition Map\n",
      "------------------\n",
      "From State InventoryState(on_hand=0, on_order=0):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -10.000] with Probability 1.000\n",
      "  With Action 1:\n",
      "    To [State InventoryState(on_hand=0, on_order=1) and Reward -10.000] with Probability 1.000\n",
      "  With Action 2:\n",
      "    To [State InventoryState(on_hand=0, on_order=2) and Reward -10.000] with Probability 1.000\n",
      "From State InventoryState(on_hand=0, on_order=1):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=1, on_order=0) and Reward -0.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -3.679] with Probability 0.632\n",
      "  With Action 1:\n",
      "    To [State InventoryState(on_hand=1, on_order=1) and Reward -0.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=1) and Reward -3.679] with Probability 0.632\n",
      "From State InventoryState(on_hand=0, on_order=2):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=2, on_order=0) and Reward -0.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=1, on_order=0) and Reward -0.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -1.036] with Probability 0.264\n",
      "From State InventoryState(on_hand=1, on_order=0):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=1, on_order=0) and Reward -1.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -4.679] with Probability 0.632\n",
      "  With Action 1:\n",
      "    To [State InventoryState(on_hand=1, on_order=1) and Reward -1.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=1) and Reward -4.679] with Probability 0.632\n",
      "From State InventoryState(on_hand=1, on_order=1):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=2, on_order=0) and Reward -1.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=1, on_order=0) and Reward -1.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -2.036] with Probability 0.264\n",
      "From State InventoryState(on_hand=2, on_order=0):\n",
      "  With Action 0:\n",
      "    To [State InventoryState(on_hand=2, on_order=0) and Reward -2.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=1, on_order=0) and Reward -2.000] with Probability 0.368\n",
      "    To [State InventoryState(on_hand=0, on_order=0) and Reward -3.036] with Probability 0.264\n",
      "\n",
      "Deterministic Policy Map\n",
      "------------------------\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 2\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n",
      "Implied MP Transition Map\n",
      "--------------\n",
      "From State InventoryState(on_hand=0, on_order=0):\n",
      "  To State InventoryState(on_hand=0, on_order=2) with Probability 1.000\n",
      "From State InventoryState(on_hand=0, on_order=1):\n",
      "  To State InventoryState(on_hand=1, on_order=1) with Probability 0.368\n",
      "  To State InventoryState(on_hand=0, on_order=1) with Probability 0.632\n",
      "From State InventoryState(on_hand=0, on_order=2):\n",
      "  To State InventoryState(on_hand=2, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=1, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=0, on_order=0) with Probability 0.264\n",
      "From State InventoryState(on_hand=1, on_order=0):\n",
      "  To State InventoryState(on_hand=1, on_order=1) with Probability 0.368\n",
      "  To State InventoryState(on_hand=0, on_order=1) with Probability 0.632\n",
      "From State InventoryState(on_hand=1, on_order=1):\n",
      "  To State InventoryState(on_hand=2, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=1, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=0, on_order=0) with Probability 0.264\n",
      "From State InventoryState(on_hand=2, on_order=0):\n",
      "  To State InventoryState(on_hand=2, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=1, on_order=0) with Probability 0.368\n",
      "  To State InventoryState(on_hand=0, on_order=0) with Probability 0.264\n",
      "\n",
      "Implied MRP Transition Reward Map\n",
      "---------------------\n",
      "From State InventoryState(on_hand=0, on_order=0):\n",
      "  To [State InventoryState(on_hand=0, on_order=2) and Reward -10.000] with Probability 1.000\n",
      "From State InventoryState(on_hand=0, on_order=1):\n",
      "  To [State InventoryState(on_hand=1, on_order=1) and Reward -0.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=0, on_order=1) and Reward -3.679] with Probability 0.632\n",
      "From State InventoryState(on_hand=0, on_order=2):\n",
      "  To [State InventoryState(on_hand=2, on_order=0) and Reward -0.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=1, on_order=0) and Reward -0.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=0, on_order=0) and Reward -1.036] with Probability 0.264\n",
      "From State InventoryState(on_hand=1, on_order=0):\n",
      "  To [State InventoryState(on_hand=1, on_order=1) and Reward -1.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=0, on_order=1) and Reward -4.679] with Probability 0.632\n",
      "From State InventoryState(on_hand=1, on_order=1):\n",
      "  To [State InventoryState(on_hand=2, on_order=0) and Reward -1.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=1, on_order=0) and Reward -1.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=0, on_order=0) and Reward -2.036] with Probability 0.264\n",
      "From State InventoryState(on_hand=2, on_order=0):\n",
      "  To [State InventoryState(on_hand=2, on_order=0) and Reward -2.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=1, on_order=0) and Reward -2.000] with Probability 0.368\n",
      "  To [State InventoryState(on_hand=0, on_order=0) and Reward -3.036] with Probability 0.264\n",
      "\n",
      "Implied MP Stationary Distribution\n",
      "-----------------------\n",
      "{InventoryState(on_hand=0, on_order=0): 0.117,\n",
      " InventoryState(on_hand=0, on_order=1): 0.279,\n",
      " InventoryState(on_hand=0, on_order=2): 0.117,\n",
      " InventoryState(on_hand=1, on_order=0): 0.162,\n",
      " InventoryState(on_hand=1, on_order=1): 0.162,\n",
      " InventoryState(on_hand=2, on_order=0): 0.162}\n",
      "\n",
      "Implied MRP Reward Function\n",
      "---------------\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -10.0,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -2.325,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -0.274,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -3.325,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -1.274,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -2.274}\n",
      "\n",
      "Implied MRP Value Function\n",
      "--------------\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -35.511,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.932,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.345,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.932,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.345,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.345}\n",
      "\n",
      "Implied MRP Policy Evaluation Value Function\n",
      "--------------\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -35.510518165628724,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.93217421014731,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.345029758390766,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.93217421014731,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.345029758390766,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.345029758390766}\n",
      "\n",
      "MDP Policy Iteration Optimal Value Function and Optimal Policy\n",
      "--------------\n",
      "{NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.660960231637507,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.991900091403533,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.660960231637507,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.894855781630035,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -29.991900091403533,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.991900091403533}\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n",
      "\n",
      "MDP Value Iteration Optimal Value Function and Optimal Policy\n",
      "--------------\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.894855194671294,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.66095964467877,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.99189950444479,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.66095964467877,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.99189950444479,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -29.991899504444792}\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from dataclasses import dataclass\n",
    "from typing import Tuple, Dict, Mapping\n",
    "from rl.markov_decision_process import FiniteMarkovDecisionProcess\n",
    "from rl.policy import FiniteDeterministicPolicy\n",
    "from rl.markov_process import FiniteMarkovProcess, FiniteMarkovRewardProcess\n",
    "from rl.distribution import Categorical\n",
    "from scipy.stats import poisson\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class InventoryState:\n",
    "    on_hand: int\n",
    "    on_order: int\n",
    "\n",
    "    def inventory_position(self) -> int:\n",
    "        return self.on_hand + self.on_order\n",
    "\n",
    "\n",
    "InvOrderMapping = Mapping[\n",
    "    InventoryState,\n",
    "    Mapping[int, Categorical[Tuple[InventoryState, float]]]\n",
    "]\n",
    "\n",
    "\n",
    "class SimpleInventoryMDPCap(FiniteMarkovDecisionProcess[InventoryState, int]):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        capacity: int,\n",
    "        poisson_lambda: float,\n",
    "        holding_cost: float,\n",
    "        stockout_cost: float\n",
    "    ):\n",
    "        self.capacity: int = capacity\n",
    "        self.poisson_lambda: float = poisson_lambda\n",
    "        self.holding_cost: float = holding_cost\n",
    "        self.stockout_cost: float = stockout_cost\n",
    "\n",
    "        self.poisson_distr = poisson(poisson_lambda)\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "\n",
    "    def get_action_transition_reward_map(self) -> InvOrderMapping:\n",
    "        d: Dict[InventoryState, Dict[int, Categorical[Tuple[InventoryState,\n",
    "                                                            float]]]] = {}\n",
    "\n",
    "        for alpha in range(self.capacity + 1):\n",
    "            for beta in range(self.capacity + 1 - alpha):\n",
    "                state: InventoryState = InventoryState(alpha, beta)\n",
    "                ip: int = state.inventory_position()\n",
    "                base_reward: float = - self.holding_cost * alpha\n",
    "                d1: Dict[int, Categorical[Tuple[InventoryState, float]]] = {}\n",
    "\n",
    "                for order in range(self.capacity - ip + 1):\n",
    "                    sr_probs_dict: Dict[Tuple[InventoryState, float], float] =\\\n",
    "                        {(InventoryState(ip - i, order), base_reward):\n",
    "                         self.poisson_distr.pmf(i) for i in range(ip)}\n",
    "\n",
    "                    probability: float = 1 - self.poisson_distr.cdf(ip - 1)\n",
    "                    reward: float = base_reward - self.stockout_cost *\\\n",
    "                        (probability * (self.poisson_lambda - ip) +\n",
    "                         ip * self.poisson_distr.pmf(ip))\n",
    "                    sr_probs_dict[(InventoryState(0, order), reward)] = \\\n",
    "                        probability\n",
    "                    d1[order] = Categorical(sr_probs_dict)\n",
    "\n",
    "                d[state] = d1\n",
    "        return d\n",
    "\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TwoStoreInventoryState:\n",
    "    on_hand_1: int\n",
    "    on_order_1: int\n",
    "    on_hand_2: int\n",
    "    on_order_2: int\n",
    "\n",
    "    def inventory_position(self) -> Tuple[int,int]:\n",
    "        return self.on_hand_1 + self.on_order_1, self.on_hand_2 + self.on_order_2\n",
    "\n",
    "\n",
    "InvOrderMapping = Mapping[\n",
    "    TwoStoreInventoryState,\n",
    "    Mapping[int, Categorical[Tuple[TwoStoreInventoryState, float]]]\n",
    "]\n",
    "\n",
    "class TwoStoreInventory(FiniteMarkovDecisionProcess[TwoStoreInventoryState, int]):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        capacity_1: int,\n",
    "        capacity_2: int,\n",
    "        poisson_lambda_1: float,\n",
    "        poisson_lambda_2: float,\n",
    "        holding_cost_1: float,\n",
    "        holding_cost_2: float,\n",
    "        stockout_cost_1: float,\n",
    "        stockout_cost_2: float,\n",
    "        supplier_cost: float,\n",
    "        between_stores_cost: float\n",
    "    ):\n",
    "        self.capacity_1: int = capacity_1\n",
    "        self.capacity_2: int = capacity_2\n",
    "        self.poisson_lambda_1: float = poisson_lambda_1\n",
    "        self.poisson_lambda_2: float = poisson_lambda_2\n",
    "        self.holding_cost_1: float = holding_cost_1\n",
    "        self.holding_cost_2: float = holding_cost_2\n",
    "        self.stockout_cost_1: float = stockout_cost_1\n",
    "        self.stockout_cost_2: float = stockout_cost_2\n",
    "        self.supplier_cost: float = supplier_cost\n",
    "        self.between_stores_cost: float = between_stores_cost\n",
    "\n",
    "        self.poisson_distr_1 = poisson(poisson_lambda_1)\n",
    "        self.poisson_distr_2 = poisson(poisson_lambda_2)\n",
    "        super().__init__(self.get_action_transition_reward_map())\n",
    "\n",
    "    def get_action_transition_reward_map(self) -> InvOrderMapping:\n",
    "        d: Dict[TwoStoreInventoryState, Dict[int, Categorical[Tuple[TwoStoreInventoryState,\n",
    "                                                            float]]]] = {}\n",
    "\n",
    "\n",
    "        for alpha1 in range(self.capacity_1+1):\n",
    "            for beta1 in range(self.capacity_1+1-alpha1):\n",
    "                for alpha2 in range(self.capacity_2+1):\n",
    "                    for beta2 in range(self.capacity_2+1-alpha2):\n",
    "                        state: TwoStoreInventory = TwoStoreInventory(alpha1,beta1,alpha2,beta2)\n",
    "                        ip1,ip2 = state.inventory_position()\n",
    "                        for gamma in range(max(-alpha2,alpha1-self.capacity_1),min(alpha1,self.capacity_2-alpha2)+1):\n",
    "                            base_reward: float = -self.holding_cost_1 * alpha1 -self.holding_cost_2 * alpha2\n",
    "                            ip1 -= gamma\n",
    "                            ip2 += gamma\n",
    "                            d1: Dict[Tuple[int,int,int], Categorical[Tuple[TwoStoreInventoryState, float]]] = {}\n",
    "                            for order1 in range(self.capacity_1-ip1+1):\n",
    "                                for order2 in range(self.capacity_2-ip2+1):\n",
    "                                    pr1: float = 1 - self.poisson_distr_1.cdf(ip1 - 1)   \n",
    "                                    pr2: float = 1 - self.poisson_distr_2.cdf(ip2 - 1)                                 \n",
    "                                    sr_probs_dict: Dict[Tuple[TwoStoreInventoryState, float], float] = {}\n",
    "                                    for index1 in range(ip1):\n",
    "                                        for index2 in range(ip2):\n",
    "                                            reward = -self.holding_cost_1*alpha1-self.holding_cost_2*alpha2\n",
    "                                            sr_probs_dict[(TwoStoreInventoryState(ip1-index1,order1,ip2-index2,order2),reward)] = self.poisson_distr_1.pmf(index1)*self.poisson_distr_2.pmf(index2)\n",
    "                                    for index1 in range(ip1):\n",
    "                                        reward = -self.holding_cost_1*alpha1-self.holding_cost_2*alpha2-self.stockout_cost_2*(pr2*(self.poisson_lambda_2-ip2)+ip2*self.poisson_distr_2.pmf(ip2))\n",
    "                                        sr_probs_dict[(TwoStoreInventoryState(ip1-index1,order1,0,order2),reward)] = self.poisson_distr_1.pmf(index1)*pr2\n",
    "                                    for index2 in range(ip2):\n",
    "                                        reward = -self.holding_cost_1*alpha1-self.holding_cost_2*alpha2-self.stockout_cost_1*(pr1*(self.poisson_lambda_1-ip1)+ip1*self.poisson_distr_1.pmf(ip1))\n",
    "                                        sr_probs_dict[(TwoStoreInventoryState(0,order1,ip2-index2,order2),reward)] = pr1*self.poisson_distr_2.pmf(index2)\n",
    "                                    reward = -self.holding_cost_1*alpha1-self.holding_cost_2*alpha2-\\\n",
    "                                        self.stockout_cost_1*(pr1*(self.poisson_lambda_1-ip1)+ip1*self.poisson_distr_1.pmf(ip1))-self.stockout_cost_2*(pr2*(self.poisson_lambda_2-ip2)+ip2*self.poisson_distr_2.pmf(ip2))\n",
    "                                    sr_probs_dict[(TwoStoreInventoryState(0,order1,0,order2),reward)]\n",
    "                            d1[(gamma,order1,order2)] = Categorical(sr_probs_dict)\n",
    "                        d[state] = d1\n",
    "\n",
    "\n",
    "        # for alpha in range(self.capacity + 1):\n",
    "        #     for beta in range(self.capacity + 1 - alpha):\n",
    "        #         state: InventoryState = InventoryState(alpha, beta)\n",
    "        #         ip: int = state.inventory_position()\n",
    "        #         base_reward: float = - self.holding_cost * alpha\n",
    "        #         d1: Dict[int, Categorical[Tuple[InventoryState, float]]] = {}\n",
    "\n",
    "        #         for order in range(self.capacity - ip + 1):\n",
    "        #             sr_probs_dict: Dict[Tuple[InventoryState, float], float] =\\\n",
    "        #                 {(InventoryState(ip - i, order), base_reward):\n",
    "        #                  self.poisson_distr.pmf(i) for i in range(ip)}\n",
    "\n",
    "        #             probability: float = 1 - self.poisson_distr.cdf(ip - 1)\n",
    "        #             reward: float = base_reward - self.stockout_cost *\\\n",
    "        #                 (probability * (self.poisson_lambda - ip) +\n",
    "        #                  ip * self.poisson_distr.pmf(ip))\n",
    "        #             sr_probs_dict[(InventoryState(0, order), reward)] = \\\n",
    "        #                 probability\n",
    "        #             d1[order] = Categorical(sr_probs_dict)\n",
    "\n",
    "        #         d[state] = d1\n",
    "        return d\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    from pprint import pprint\n",
    "\n",
    "    user_capacity = 2\n",
    "    user_poisson_lambda = 1.0\n",
    "    user_holding_cost = 1.0\n",
    "    user_stockout_cost = 10.0\n",
    "\n",
    "    user_gamma = 0.9\n",
    "\n",
    "    si_mdp: FiniteMarkovDecisionProcess[InventoryState, int] =\\\n",
    "        SimpleInventoryMDPCap(\n",
    "            capacity=user_capacity,\n",
    "            poisson_lambda=user_poisson_lambda,\n",
    "            holding_cost=user_holding_cost,\n",
    "            stockout_cost=user_stockout_cost\n",
    "        )\n",
    "\n",
    "    print(\"MDP Transition Map\")\n",
    "    print(\"------------------\")\n",
    "    print(si_mdp)\n",
    "\n",
    "    fdp: FiniteDeterministicPolicy[InventoryState, int] = \\\n",
    "        FiniteDeterministicPolicy(\n",
    "            {InventoryState(alpha, beta): user_capacity - (alpha + beta)\n",
    "             for alpha in range(user_capacity + 1)\n",
    "             for beta in range(user_capacity + 1 - alpha)}\n",
    "    )\n",
    "\n",
    "    print(\"Deterministic Policy Map\")\n",
    "    print(\"------------------------\")\n",
    "    print(fdp)\n",
    "\n",
    "    implied_mrp: FiniteMarkovRewardProcess[InventoryState] =\\\n",
    "        si_mdp.apply_finite_policy(fdp)\n",
    "    print(\"Implied MP Transition Map\")\n",
    "    print(\"--------------\")\n",
    "    print(FiniteMarkovProcess(\n",
    "        {s.state: Categorical({s1.state: p for s1, p in v.table().items()})\n",
    "         for s, v in implied_mrp.transition_map.items()}\n",
    "    ))\n",
    "\n",
    "    print(\"Implied MRP Transition Reward Map\")\n",
    "    print(\"---------------------\")\n",
    "    print(implied_mrp)\n",
    "\n",
    "    print(\"Implied MP Stationary Distribution\")\n",
    "    print(\"-----------------------\")\n",
    "    implied_mrp.display_stationary_distribution()\n",
    "    print()\n",
    "\n",
    "    print(\"Implied MRP Reward Function\")\n",
    "    print(\"---------------\")\n",
    "    implied_mrp.display_reward_function()\n",
    "    print()\n",
    "\n",
    "    print(\"Implied MRP Value Function\")\n",
    "    print(\"--------------\")\n",
    "    implied_mrp.display_value_function(gamma=user_gamma)\n",
    "    print()\n",
    "\n",
    "    from rl.dynamic_programming import evaluate_mrp_result\n",
    "    from rl.dynamic_programming import policy_iteration_result\n",
    "    from rl.dynamic_programming import value_iteration_result\n",
    "\n",
    "    print(\"Implied MRP Policy Evaluation Value Function\")\n",
    "    print(\"--------------\")\n",
    "    pprint(evaluate_mrp_result(implied_mrp, gamma=user_gamma))\n",
    "    print()\n",
    "\n",
    "    print(\"MDP Policy Iteration Optimal Value Function and Optimal Policy\")\n",
    "    print(\"--------------\")\n",
    "    opt_vf_pi, opt_policy_pi = policy_iteration_result(\n",
    "        si_mdp,\n",
    "        gamma=user_gamma\n",
    "    )\n",
    "    pprint(opt_vf_pi)\n",
    "    print(opt_policy_pi)\n",
    "    print()\n",
    "\n",
    "    print(\"MDP Value Iteration Optimal Value Function and Optimal Policy\")\n",
    "    print(\"--------------\")\n",
    "    opt_vf_vi, opt_policy_vi = value_iteration_result(si_mdp, gamma=user_gamma)\n",
    "    pprint(opt_vf_vi)\n",
    "    print(opt_policy_vi)\n",
    "    print()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96eb50e1d44aed467dc8f759cb08c32fbfa9babcf79c554e2d0e5feb04653a10"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

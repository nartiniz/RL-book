{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------- MONTE CARLO VALUE FUNCTION --------------\n",
      "{'B': 5.642857142857143, 'A': 9.571428571428571}\n",
      "-------------- MRP VALUE FUNCTION ----------\n",
      "{'A': 12.933333333333323, 'T': 0.0, 'B': 9.599999999999994}\n",
      "------------- TD VALUE FUNCTION --------------\n",
      "td_vf [13.72070364 10.69307161]\n"
     ]
    }
   ],
   "source": [
    "from typing import Sequence, Tuple, Mapping\n",
    "from typing import Sequence, Iterable, Callable\n",
    "from rl.function_approx import AdamGradient\n",
    "from rl.function_approx import LinearFunctionApprox\n",
    "from rl.approximate_dynamic_programming import ValueFunctionApprox\n",
    "from rl.distribution import Choose\n",
    "from rl.markov_decision_process import NonTerminal\n",
    "from rl.chapter2.simple_inventory_mrp import SimpleInventoryMRPFinite\n",
    "from rl.chapter2.simple_inventory_mrp import InventoryState\n",
    "from rl.chapter10.prediction_utils import (\n",
    "    mc_prediction_learning_rate,\n",
    "    td_prediction_learning_rate\n",
    ")\n",
    "import numpy as np\n",
    "from itertools import islice\n",
    "from rl.policy import *\n",
    "from rl.distribution import (Categorical, Distribution, FiniteDistribution,\n",
    "                             SampledDistribution)\n",
    "from rl.markov_process import *\n",
    "\n",
    "\n",
    "S = str\n",
    "DataType = Sequence[Sequence[Tuple[S, float]]]\n",
    "ProbFunc = Mapping[S, Mapping[S, float]]\n",
    "RewardFunc = Mapping[S, float]\n",
    "ValueFunc = Mapping[S, float]\n",
    "\n",
    "\n",
    "def get_state_return_samples(\n",
    "    data: DataType\n",
    ") -> Sequence[Tuple[S, float]]:\n",
    "    \"\"\"\n",
    "    prepare sequence of (state, return) pairs.\n",
    "    Note: (state, return) pairs is not same as (state, reward) pairs.\n",
    "    \"\"\"\n",
    "    return [(s, sum(r for (_, r) in l[i:]))\n",
    "            for l in data for i, (s, _) in enumerate(l)]\n",
    "\n",
    "\n",
    "def get_mc_value_function(\n",
    "    state_return_samples: Sequence[Tuple[S, float]]\n",
    ") -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement tabular MC Value Function compatible with the interface defined above.\n",
    "    \"\"\"\n",
    "    ###### MY IMPLEMENTATION #######\n",
    "    ################################\n",
    "    all_states = set(x[0] for x in state_return_samples)\n",
    "    appearances : Mapping={}\n",
    "    summations: Mapping={}\n",
    "    for state in all_states:\n",
    "        appearances[state]=0\n",
    "        summations[state]=0\n",
    "    for state,ret in state_return_samples:\n",
    "        appearances[state] += 1\n",
    "        summations[state] += ret\n",
    "    value_func:Mapping={}\n",
    "    for state in all_states:\n",
    "        value_func[state] = summations[state]/appearances[state]\n",
    "    return value_func\n",
    "\n",
    "def get_state_reward_next_state_samples(\n",
    "    data: DataType\n",
    ") -> Sequence[Tuple[S, float, S]]:\n",
    "    \"\"\"\n",
    "    prepare sequence of (state, reward, next_state) triples.\n",
    "    \"\"\"\n",
    "    return [(s, r, l[i+1][0] if i < len(l) - 1 else 'T')\n",
    "            for l in data for i, (s, r) in enumerate(l)]\n",
    "\n",
    "\n",
    "def get_probability_and_reward_functions(\n",
    "    srs_samples: Sequence[Tuple[S, float, S]]\n",
    ") -> Tuple[ProbFunc, RewardFunc]:\n",
    "    \"\"\"\n",
    "    Implement code that produces the probability transitions and the\n",
    "    reward function compatible with the interface defined above.\n",
    "    \"\"\"\n",
    "    all_states = set(x[0] for x in srs_samples).union(set(x[2] for x in srs_samples))\n",
    "    appearance:Mapping={}\n",
    "    for state in all_states:\n",
    "        appearance[state] = 0\n",
    "    for x in srs_samples:\n",
    "        appearance[x[0]] += 1\n",
    "    \n",
    "    prob:Mapping[S, Mapping[S, float]] = {}\n",
    "    for state1 in all_states:\n",
    "        prob[state1] = {}\n",
    "        for state2 in all_states:\n",
    "            prob[state1][state2] = 0\n",
    "    for x in srs_samples:\n",
    "        prob[x[0]][x[2]] += 1/appearance[x[0]]\n",
    "\n",
    "    rew:Mapping[S, float] = {}\n",
    "    for state in all_states:\n",
    "        rew[state]=0\n",
    "    for x in srs_samples:\n",
    "        rew[x[0]] += x[1] / appearance[x[0]]\n",
    "    return prob,rew \n",
    "\n",
    "\n",
    "def get_mrp_value_function(\n",
    "    prob_func: ProbFunc,\n",
    "    reward_func: RewardFunc\n",
    ") -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement code that calculates the MRP Value Function from the probability\n",
    "    transitions and reward function, compatible with the interface defined above.\n",
    "    Hint: Use the MRP Bellman Equation and simple linear algebra\n",
    "    \"\"\"\n",
    "    all_states = set(x[0] for x in reward_func.keys())\n",
    "    all_states = list(all_states)\n",
    "    n_state = len(all_states)\n",
    "    P = np.zeros((n_state,n_state))\n",
    "    R = np.zeros((n_state))\n",
    "    for i,state in enumerate(all_states):\n",
    "        P[i]\n",
    "    for i in range(n_state):\n",
    "        R[i] = reward_func[all_states[i]]\n",
    "        for j in range(n_state):\n",
    "            P[i,j] = prob_func[all_states[i]][all_states[j]]\n",
    "    vstar = np.linalg.inv(np.eye(n_state)-P)@R\n",
    "    vf: Mapping[S,float] = {}\n",
    "    for i in range(n_state):\n",
    "        vf[all_states[i]] = vstar[i]\n",
    "    return vf\n",
    "\n",
    "\n",
    "\n",
    "def get_td_value_function(\n",
    "    srs_samples: Sequence[Tuple[S, float, S]],\n",
    "    num_updates: int = 300000,\n",
    "    learning_rate: float = 0.3,\n",
    "    learning_rate_decay: int = 30\n",
    ") -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement tabular TD(0) (with experience replay) Value Function compatible\n",
    "    with the interface defined above. Let the step size (alpha) be:\n",
    "    learning_rate * (updates / learning_rate_decay + 1) ** -0.5\n",
    "    so that Robbins-Monro condition is satisfied for the sequence of step sizes.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    ag = AdamGradient(\n",
    "    learning_rate=0.05,\n",
    "    decay1=0.9,\n",
    "    decay2=0.999\n",
    "    )\n",
    "    all_state_names = set(x[0] for x in srs_samples)\n",
    "    \n",
    "    # ffs: Sequence[Callable[[NonTerminal[InventoryState]], float]] = \\\n",
    "    # [(lambda x, s=s: float(x.state == s.state)) for s in all_states]\n",
    "\n",
    "    appearance={}\n",
    "    for state in all_state_names:\n",
    "        appearance[state] = 0\n",
    "    for x in srs_samples:\n",
    "        appearance[x[0]] += 1\n",
    "\n",
    "    d={}\n",
    "    for state in all_state_names:\n",
    "        d[state]={}\n",
    "    for x in srs_samples:\n",
    "        d[x[0]][(x[2],x[1])]=1/appearance[x[0]]\n",
    "\n",
    "    for state in all_state_names:\n",
    "        d[state] = Categorical(d[state])\n",
    "    # print(d)\n",
    "    mrp=FiniteMarkovRewardProcess(d)\n",
    "    # print(mrp.get_value_function_vec(gamma=1))\n",
    "    # print(mrp.transition_map)\n",
    "    td_episode_length = 1000\n",
    "    gamma = 1\n",
    "    all_states = mrp.non_terminal_states\n",
    "    # print(all_states)\n",
    "    ffs: Sequence[Callable[[NonTerminal[S]], float]] = \\\n",
    "    [(lambda x, s=s: float(x.state == s.state)) for s in all_states]\n",
    "    \n",
    "    lfa = LinearFunctionApprox.create(\n",
    "        feature_functions=ffs,\n",
    "        adam_gradient=ag,\n",
    "        regularization_coeff=0.0001,\n",
    "        direct_solve=False\n",
    ")\n",
    "    lfa = LinearFunctionApprox.create(\n",
    "            feature_functions=ffs,\n",
    "            adam_gradient=ag,\n",
    "            direct_solve=False\n",
    "    )\n",
    "    it_td: Iterable[ValueFunctionApprox[S]] = \\\n",
    "        td_prediction_learning_rate(\n",
    "            mrp=mrp,\n",
    "            start_state_distribution=Choose(all_states),\n",
    "            gamma=gamma,\n",
    "            episode_length=td_episode_length,\n",
    "            initial_func_approx=lfa\n",
    "        )\n",
    "\n",
    "    \n",
    "\n",
    "    td_experiences: int = 5000\n",
    "    for i, td_vf in enumerate(islice(it_td, td_experiences)):\n",
    "        zzz=3\n",
    "        # td_rmse: float = np.sqrt(sum(\n",
    "        #     (td_vf(s) - true_vf[i]) ** 2 for i, s in enumerate(all_states)\n",
    "        # ) / len(all_states))\n",
    "        # if i%2000==0:\n",
    "        #     print(f\"TD: Iteration = {i:d}\")\n",
    "            # print(f\"TD: Iteration = {i:d}, RMSE = {td_rmse:.3f}\")\n",
    "    # print(td_vf)\n",
    "    print(f\"td_vf {np.flip(td_vf.weights.weights)}\")\n",
    "    # print(f\"True VF : {true_vf}\")\n",
    "\n",
    "\n",
    "def get_lstd_value_function(\n",
    "    srs_samples: Sequence[Tuple[S, float, S]]\n",
    ") -> ValueFunc:\n",
    "    \"\"\"\n",
    "    Implement LSTD Value Function compatible with the interface defined above.\n",
    "    Hint: Tabular is a special case of linear function approx where each feature\n",
    "    is an indicator variables for a corresponding state and each parameter is\n",
    "    the value function for the corresponding state.\n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    given_data: DataType = [\n",
    "        [('A', 2.), ('A', 6.), ('B', 1.), ('B', 2.)],\n",
    "        [('A', 3.), ('B', 2.), ('A', 4.), ('B', 2.), ('B', 0.)],\n",
    "        [('B', 3.), ('B', 6.), ('A', 1.), ('B', 1.)],\n",
    "        [('A', 0.), ('B', 2.), ('A', 4.), ('B', 4.), ('B', 2.), ('B', 3.)],\n",
    "        [('B', 8.), ('B', 2.)]\n",
    "    ]\n",
    "\n",
    "    sr_samps = get_state_return_samples(given_data)\n",
    "\n",
    "    print(\"------------- MONTE CARLO VALUE FUNCTION --------------\")\n",
    "    print(get_mc_value_function(sr_samps))\n",
    "\n",
    "    srs_samps = get_state_reward_next_state_samples(given_data)\n",
    "\n",
    "    pfunc, rfunc = get_probability_and_reward_functions(srs_samps)\n",
    "    print(\"-------------- MRP VALUE FUNCTION ----------\")\n",
    "    print(get_mrp_value_function(pfunc, rfunc))\n",
    "\n",
    "    print(\"------------- TD VALUE FUNCTION --------------\")\n",
    "    # print(get_td_value_function(srs_samps))\n",
    "    (get_td_value_function(srs_samps))\n",
    "\n",
    "\n",
    "    ##### I COULD NOT FINISH THE LSTD PART\n",
    "    #  \n",
    "    # print(\"------------- LSTD VALUE FUNCTION --------------\")\n",
    "    # print(get_lstd_value_function(srs_samps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'td_vf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-e45567d85b3e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtd_vf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'td_vf' is not defined"
     ]
    }
   ],
   "source": [
    "print(td_vf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96eb50e1d44aed467dc8f759cb08c32fbfa9babcf79c554e2d0e5feb04653a10"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

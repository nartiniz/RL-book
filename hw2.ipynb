{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "# import graphviz\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from typing import (Callable, Dict, Iterable, Generic, Sequence, Tuple,\n",
    "                    Mapping, TypeVar, Set)\n",
    "\n",
    "from rl.distribution import (Categorical, Distribution, FiniteDistribution,\n",
    "                             SampledDistribution)\n",
    "\n",
    "S = TypeVar('S')\n",
    "X = TypeVar('X')\n",
    "\n",
    "class State(ABC, Generic[S]):\n",
    "    state: S\n",
    "\n",
    "    def on_non_terminal(\n",
    "        self,\n",
    "        f: Callable[[NonTerminal[S]], X],\n",
    "        default: X\n",
    "    ) -> X:\n",
    "        if isinstance(self, NonTerminal):\n",
    "            return f(self)\n",
    "        else:\n",
    "            return default\n",
    "\n",
    "    \n",
    "@dataclass(frozen=True)\n",
    "class Terminal(State[S]):\n",
    "    state: S\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class NonTerminal(State[S]):\n",
    "    state: S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovProcess(ABC, Generic[S]):\n",
    "    '''A Markov process with states of type S.\n",
    "    '''\n",
    "    @abstractmethod\n",
    "    def transition(self, state: NonTerminal[S]) -> Distribution[State[S]]:\n",
    "        '''Given a state of the process, returns a distribution of\n",
    "        the next states.  Returning None means we are in a terminal state.\n",
    "        '''\n",
    "\n",
    "    def simulate(\n",
    "        self,\n",
    "        start_state_distribution: Distribution[NonTerminal[S]]\n",
    "    ) -> Iterable[State[S]]:\n",
    "        '''Run a simulation trace of this Markov process, generating the\n",
    "        states visited during the trace.\n",
    "\n",
    "        This yields the start state first, then continues yielding\n",
    "        subsequent states forever or until we hit a terminal state.\n",
    "        '''\n",
    "        state: State[S] = start_state_distribution.sample()\n",
    "        # print(start_state_distribution.sample())\n",
    "        yield state\n",
    "        # print(state)\n",
    "        while isinstance(state, NonTerminal):\n",
    "            \n",
    "            # print(self.transition(state))\n",
    "            state = self.transition(state).sample()\n",
    "            # print(state)\n",
    "            yield state\n",
    "\n",
    "    def traces(\n",
    "            self,\n",
    "            start_state_distribution: Distribution[NonTerminal[S]]\n",
    "    ) -> Iterable[Iterable[State[S]]]:\n",
    "        '''Yield simulation traces (the output of `simulate'), sampling a\n",
    "        start state from the given distribution each time.\n",
    "\n",
    "        '''\n",
    "        while True:\n",
    "            yield self.simulate(start_state_distribution)\n",
    "\n",
    "\n",
    "Transition = Mapping[NonTerminal[S], FiniteDistribution[State[S]]]\n",
    "\n",
    "\n",
    "class FiniteMarkovProcess(MarkovProcess[S]):\n",
    "    '''A Markov Process with a finite state space.\n",
    "\n",
    "    Having a finite state space lets us use tabular methods to work\n",
    "    with the process (ie dynamic programming).\n",
    "\n",
    "    '''\n",
    "\n",
    "    non_terminal_states: Sequence[NonTerminal[S]]\n",
    "    transition_map: Transition[S]\n",
    "\n",
    "\n",
    "    def __init__(self, transition_map: Mapping[S, FiniteDistribution[S]]):\n",
    "        non_terminals: Set[S] = set(transition_map.keys())\n",
    "        self.transition_map = {\n",
    "            NonTerminal(s): Categorical(\n",
    "                {(s1 if s1 in non_terminals else s1): p\n",
    "                 for s1, p in v.table().items()}\n",
    "            ) for s, v in transition_map.items()\n",
    "        }\n",
    "        self.non_terminal_states = list(self.transition_map.keys())\n",
    "\n",
    "    ####ORIGINAL ONE IS BELOW #####\n",
    "    # def __init__(self, transition_map: Mapping[S, FiniteDistribution[S]]):\n",
    "    #     non_terminals: Set[S] = set(transition_map.keys())\n",
    "    #     self.transition_map = {\n",
    "    #         NonTerminal(s): Categorical(\n",
    "    #             {(NonTerminal(s1) if s1 in non_terminals else Terminal(s1)): p\n",
    "    #              for s1, p in v.table().items()}\n",
    "    #         ) for s, v in transition_map.items()\n",
    "    #     }\n",
    "    #     self.non_terminal_states = list(self.transition_map.keys())\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        display = \"\"\n",
    "\n",
    "        for s, d in self.transition_map.items():\n",
    "            display += f\"From State {s.state}:\\n\"\n",
    "            for s1, p in d:\n",
    "                opt = \"Terminal \" if isinstance(s1, Terminal) else \"\"\n",
    "                display += f\"  To {opt}State {s1.state} with Probability {p:.3f}\\n\"\n",
    "\n",
    "        return display\n",
    "\n",
    "    def get_transition_matrix(self) -> np.ndarray:\n",
    "        sz = len(self.non_terminal_states)\n",
    "        mat = np.zeros((sz, sz))\n",
    "\n",
    "        for i, s1 in enumerate(self.non_terminal_states):\n",
    "            for j, s2 in enumerate(self.non_terminal_states):\n",
    "                mat[i, j] = self.transition(s1).probability(s2)\n",
    "        return mat\n",
    "\n",
    "\n",
    "    def transition(self, state: NonTerminal[S])\\\n",
    "            -> FiniteDistribution[State[S]]:\n",
    "        return self.transition_map[state]\n",
    "\n",
    "    def get_stationary_distribution(self) -> FiniteDistribution[S]:\n",
    "        eig_vals, eig_vecs = np.linalg.eig(self.get_transition_matrix().T)\n",
    "        index_of_first_unit_eig_val = np.where(\n",
    "            np.abs(eig_vals - 1) < 1e-8)[0][0]\n",
    "        eig_vec_of_unit_eig_val = np.real(\n",
    "            eig_vecs[:, index_of_first_unit_eig_val])\n",
    "        return Categorical({\n",
    "            self.non_terminal_states[i].state: ev\n",
    "            for i, ev in enumerate(eig_vec_of_unit_eig_val /\n",
    "                                   sum(eig_vec_of_unit_eig_val))\n",
    "        })\n",
    "\n",
    "    def display_stationary_distribution(self):\n",
    "        pprint({\n",
    "            s: round(p, 3)\n",
    "            for s, p in self.get_stationary_distribution()\n",
    "        })\n",
    "\n",
    "    def generate_image(self) -> graphviz.Digraph:\n",
    "        d = graphviz.Digraph()\n",
    "\n",
    "        for s in self.transition_map.keys():\n",
    "            d.node(str(s))\n",
    "\n",
    "        for s, v in self.transition_map.items():\n",
    "            for s1, p in v:\n",
    "                d.edge(str(s), str(s1), label=str(p))\n",
    "\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NonTerminal(state=3)\n",
      "NonTerminal(state=7)\n",
      "NonTerminal(state=10)\n",
      "NonTerminal(state=12)\n",
      "NonTerminal(state=14)\n",
      "NonTerminal(state=16)\n",
      "NonTerminal(state=10)\n",
      "NonTerminal(state=14)\n",
      "NonTerminal(state=18)\n",
      "NonTerminal(state=20)\n",
      "NonTerminal(state=24)\n",
      "NonTerminal(state=26)\n",
      "NonTerminal(state=32)\n",
      "NonTerminal(state=36)\n",
      "NonTerminal(state=49)\n",
      "NonTerminal(state=17)\n",
      "NonTerminal(state=19)\n",
      "NonTerminal(state=21)\n",
      "NonTerminal(state=48)\n",
      "NonTerminal(state=51)\n",
      "NonTerminal(state=72)\n",
      "NonTerminal(state=73)\n",
      "NonTerminal(state=78)\n",
      "Terminal(state=80)\n"
     ]
    }
   ],
   "source": [
    "N = 10000\n",
    "States = {}\n",
    "\n",
    "for i in range(1,100):\n",
    "    States[i] = NonTerminal(i)\n",
    "for i in range(100,106):\n",
    "    States[i] = Terminal(i)\n",
    "States[80] = Terminal(80)\n",
    "# States[100] = Terminal(100)\n",
    "\n",
    "Dists: Mapping[int, float] = {}\n",
    "for i in range(0,100):\n",
    "    temp_map: Mapping[int, float] = {}\n",
    "    for j in range(1,7):\n",
    "            temp_map[States[i+j]] = 1/6\n",
    "        # if i+j<=100:\n",
    "        #     temp_map[States[i+j]] = 1/6\n",
    "    Dists[i] = Categorical(temp_map)\n",
    "\n",
    "\n",
    "Dists[1] = Dists[38]\n",
    "Dists[4] = Dists[14]\n",
    "Dists[9] = Dists[31]\n",
    "Dists[16] = Dists[6]\n",
    "Dists[21] = Dists[42]\n",
    "Dists[28] = Dists[84]\n",
    "Dists[36] = Dists[44]\n",
    "Dists[47] = Dists[26]\n",
    "Dists[49] = Dists[11]\n",
    "Dists[51] = Dists[67]\n",
    "Dists[56] = Dists[53]\n",
    "Dists[62] = Dists[19]\n",
    "Dists[64] = Dists[60]\n",
    "Dists[71] = Dists[91]\n",
    "Dists[87] = Dists[24]\n",
    "Dists[93] = Dists[73]\n",
    "Dists[95] = Dists[75]\n",
    "Dists[98] = Dists[78]\n",
    "\n",
    "\n",
    "process = FiniteMarkovProcess(Dists)\n",
    "for state in process.simulate(Dists[0]): \n",
    "    print(state)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137.42799999998743\n"
     ]
    }
   ],
   "source": [
    "######FROG GAME###########\n",
    "N=1000\n",
    "States = {}\n",
    "for i in range(1,N):\n",
    "    States[i] = NonTerminal(i)\n",
    "\n",
    "States[0] = Terminal(0)\n",
    "\n",
    "Dists: Mapping[int, float] = {}\n",
    "for i in range(1,N):\n",
    "    temp_map: Mapping[int, float] = {}\n",
    "    for j in range(0,i):\n",
    "            temp_map[States[j]] = 1/(i)\n",
    "    Dists[i] = Categorical(temp_map)\n",
    "process = FiniteMarkovProcess(Dists)\n",
    "avg = 0\n",
    "for _ in range(15000):\n",
    "    inc = 0\n",
    "    for state in process.simulate(Dists[5]): \n",
    "        inc += 1\n",
    "    avg += inc/15000\n",
    "print(avg*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reward processes\n",
    "@dataclass(frozen=True)\n",
    "class TransitionStep(Generic[S]):\n",
    "    state: NonTerminal[S]\n",
    "    next_state: State[S]\n",
    "    reward: float\n",
    "\n",
    "    def add_return(self, γ: float, return_: float) -> ReturnStep[S]:\n",
    "        '''Given a γ and the return from 'next_state', this annotates the\n",
    "        transition with a return for 'state'.\n",
    "\n",
    "        '''\n",
    "        return ReturnStep(\n",
    "            self.state,\n",
    "            self.next_state,\n",
    "            self.reward,\n",
    "            return_=self.reward + γ * return_\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ReturnStep(TransitionStep[S]):\n",
    "    return_: float\n",
    "\n",
    "\n",
    "class MarkovRewardProcess(MarkovProcess[S]):\n",
    "    def transition(self, state: NonTerminal[S]) -> Distribution[State[S]]:\n",
    "        '''Transitions the Markov Reward Process, ignoring the generated\n",
    "        reward (which makes this just a normal Markov Process).\n",
    "\n",
    "        '''\n",
    "        distribution = self.transition_reward(state)\n",
    "\n",
    "        def next_state(distribution=distribution):\n",
    "            next_s, _ = distribution.sample()\n",
    "            return next_s\n",
    "\n",
    "        return SampledDistribution(next_state)\n",
    "\n",
    "    @abstractmethod\n",
    "    def transition_reward(self, state: NonTerminal[S])\\\n",
    "            -> Distribution[Tuple[State[S], float]]:\n",
    "        '''Given a state, returns a distribution of the next state\n",
    "        and reward from transitioning between the states.\n",
    "\n",
    "        '''\n",
    "\n",
    "    def simulate_reward(\n",
    "        self,\n",
    "        start_state_distribution: Distribution[NonTerminal[S]]\n",
    "    ) -> Iterable[TransitionStep[S]]:\n",
    "        '''Simulate the MRP, yielding an Iterable of\n",
    "        (state, next state, reward) for each sampled transition.\n",
    "        '''\n",
    "\n",
    "        state: State[S] = start_state_distribution.sample()\n",
    "        reward: float = 0.\n",
    "\n",
    "        while isinstance(state, NonTerminal):\n",
    "            next_distribution = self.transition_reward(state)\n",
    "\n",
    "            next_state, reward = next_distribution.sample()\n",
    "            yield TransitionStep(state, next_state, reward)\n",
    "\n",
    "            state = next_state\n",
    "\n",
    "    def reward_traces(\n",
    "            self,\n",
    "            start_state_distribution: Distribution[NonTerminal[S]]\n",
    "    ) -> Iterable[Iterable[TransitionStep[S]]]:\n",
    "        '''Yield simulation traces (the output of `simulate_reward'), sampling\n",
    "        a start state from the given distribution each time.\n",
    "\n",
    "        '''\n",
    "        while True:\n",
    "            yield self.simulate_reward(start_state_distribution)\n",
    "\n",
    "\n",
    "StateReward = FiniteDistribution[Tuple[State[S], float]]\n",
    "RewardTransition = Mapping[NonTerminal[S], StateReward[S]]\n",
    "\n",
    "\n",
    "class FiniteMarkovRewardProcess(FiniteMarkovProcess[S],\n",
    "                                MarkovRewardProcess[S]):\n",
    "\n",
    "    transition_reward_map: RewardTransition[S]\n",
    "    reward_function_vec: np.ndarray\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        transition_reward_map: Mapping[S, FiniteDistribution[Tuple[S, float]]]\n",
    "    ):\n",
    "        transition_map: Dict[S, FiniteDistribution[S]] = {}\n",
    "\n",
    "        for state, trans in transition_reward_map.items():\n",
    "            probabilities: Dict[S, float] = defaultdict(float)\n",
    "            for (next_state, _), probability in trans:\n",
    "                probabilities[next_state] += probability\n",
    "\n",
    "            transition_map[state] = Categorical(probabilities)\n",
    "\n",
    "        super().__init__(transition_map)\n",
    "\n",
    "        nt: Set[S] = set(transition_reward_map.keys())\n",
    "        self.transition_reward_map = {\n",
    "            NonTerminal(s): Categorical(\n",
    "                {(s1 if s1 in nt else s1, r): p\n",
    "                 for (s1, r), p in v.table().items()}\n",
    "            ) for s, v in transition_reward_map.items()\n",
    "        }\n",
    "        #####ORIGINAL ONE#########\n",
    "        # nt: Set[S] = set(transition_reward_map.keys())\n",
    "        # self.transition_reward_map = {\n",
    "        #     NonTerminal(s): Categorical(\n",
    "        #         {(NonTerminal(s1) if s1 in nt else Terminal(s1), r): p\n",
    "        #          for (s1, r), p in v.table().items()}\n",
    "        #     ) for s, v in transition_reward_map.items()\n",
    "        # }\n",
    "\n",
    "        self.reward_function_vec = np.array([\n",
    "            sum(probability * reward for (_, reward), probability in\n",
    "                self.transition_reward_map[state])\n",
    "            for state in self.non_terminal_states\n",
    "        ])\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        display = \"\"\n",
    "        for s, d in self.transition_reward_map.items():\n",
    "            display += f\"From State {s.state}:\\n\"\n",
    "            for (s1, r), p in d:\n",
    "                opt = \"Terminal \" if isinstance(s1, Terminal) else \"\"\n",
    "                display +=\\\n",
    "                    f\"  To [{opt}State {s1.state} and Reward {r:.3f}]\"\\\n",
    "                    + f\" with Probability {p:.3f}\\n\"\n",
    "        return display\n",
    "\n",
    "    def transition_reward(self, state: NonTerminal[S]) -> StateReward[S]:\n",
    "        return self.transition_reward_map[state]\n",
    "\n",
    "    def get_value_function_vec(self, gamma: float) -> np.ndarray:\n",
    "        return np.linalg.solve(\n",
    "            np.eye(len(self.non_terminal_states)) -\n",
    "            gamma * self.get_transition_matrix(),\n",
    "            self.reward_function_vec\n",
    "        )\n",
    "\n",
    "    def display_reward_function(self):\n",
    "        pprint({\n",
    "            self.non_terminal_states[i]: round(r, 3)\n",
    "            for i, r in enumerate(self.reward_function_vec)\n",
    "        })\n",
    "\n",
    "    def display_value_function(self, gamma: float):\n",
    "        pprint({\n",
    "            self.non_terminal_states[i]: round(v, 3)\n",
    "            for i, v in enumerate(self.get_value_function_vec(gamma))\n",
    "        })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransitionStep(state=NonTerminal(state=3), next_state=NonTerminal(state=9), reward=0)\n",
      "TransitionStep(state=NonTerminal(state=9), next_state=NonTerminal(state=35), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=35), next_state=NonTerminal(state=37), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=37), next_state=NonTerminal(state=39), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=39), next_state=NonTerminal(state=44), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=44), next_state=NonTerminal(state=45), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=45), next_state=NonTerminal(state=47), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=47), next_state=NonTerminal(state=29), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=29), next_state=NonTerminal(state=33), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=33), next_state=NonTerminal(state=36), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=36), next_state=NonTerminal(state=50), reward=0)\n",
      "TransitionStep(state=NonTerminal(state=50), next_state=NonTerminal(state=52), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=52), next_state=NonTerminal(state=57), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=57), next_state=NonTerminal(state=59), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=59), next_state=NonTerminal(state=62), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=62), next_state=NonTerminal(state=21), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=21), next_state=NonTerminal(state=43), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=43), next_state=NonTerminal(state=44), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=44), next_state=NonTerminal(state=46), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=46), next_state=NonTerminal(state=52), reward=0)\n",
      "TransitionStep(state=NonTerminal(state=52), next_state=NonTerminal(state=57), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=57), next_state=NonTerminal(state=59), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=59), next_state=NonTerminal(state=63), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=63), next_state=NonTerminal(state=64), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=64), next_state=NonTerminal(state=63), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=63), next_state=NonTerminal(state=69), reward=0)\n",
      "TransitionStep(state=NonTerminal(state=69), next_state=NonTerminal(state=75), reward=0)\n",
      "TransitionStep(state=NonTerminal(state=75), next_state=NonTerminal(state=79), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=79), next_state=NonTerminal(state=81), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=81), next_state=NonTerminal(state=86), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=86), next_state=NonTerminal(state=90), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=90), next_state=NonTerminal(state=95), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=95), next_state=NonTerminal(state=76), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=76), next_state=NonTerminal(state=77), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=77), next_state=NonTerminal(state=78), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=78), next_state=NonTerminal(state=81), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=81), next_state=NonTerminal(state=85), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=85), next_state=NonTerminal(state=88), reward=1)\n",
      "TransitionStep(state=NonTerminal(state=88), next_state=NonTerminal(state=94), reward=0)\n",
      "TransitionStep(state=NonTerminal(state=94), next_state=Terminal(state=100), reward=0)\n",
      "33\n"
     ]
    }
   ],
   "source": [
    "#######REWARD VERSION OF SNAKES AND LADDERS#####\n",
    "States = {}\n",
    "\n",
    "for i in range(1,100):\n",
    "    States[i] = NonTerminal(i)\n",
    "for i in range(100,106):\n",
    "    States[i] = Terminal(i)\n",
    "States[80] = Terminal(80)\n",
    "# States[100] = Terminal(100)\n",
    "\n",
    "Dists: Mapping[int, float] = {}\n",
    "for i in range(0,100):\n",
    "    temp_map: Mapping[int, float] = {}\n",
    "    for j in range(1,7):\n",
    "            temp_map[States[i+j]] = 1/6\n",
    "    Dists[i] = Categorical(temp_map)\n",
    "\n",
    "Reward_map: Mapping[int,Categorical]={}\n",
    "for i in range(0,100):\n",
    "    temp_map: Mapping[tuple, float] = {}\n",
    "    for j in range(1,7):\n",
    "        if j<6:\n",
    "            temp_map[(States[i+j],1)] = 1/6\n",
    "        else: \n",
    "            temp_map[(States[i+j],0)] = 1/6\n",
    "    Reward_map[i] = Categorical(temp_map)\n",
    "\n",
    "\n",
    "Reward_map[1] = Reward_map[38]\n",
    "Reward_map[4] = Reward_map[14]\n",
    "Reward_map[9] = Reward_map[31]\n",
    "Reward_map[16] = Reward_map[6]\n",
    "Reward_map[21] = Reward_map[42]\n",
    "Reward_map[28] = Reward_map[84]\n",
    "Reward_map[36] = Reward_map[44]\n",
    "Reward_map[47] = Reward_map[26]\n",
    "Reward_map[49] = Reward_map[11]\n",
    "Reward_map[51] = Reward_map[67]\n",
    "Reward_map[56] = Reward_map[53]\n",
    "Reward_map[62] = Reward_map[19]\n",
    "Reward_map[64] = Reward_map[60]\n",
    "Reward_map[71] = Reward_map[91]\n",
    "Reward_map[87] = Reward_map[24]\n",
    "Reward_map[93] = Reward_map[73]\n",
    "Reward_map[95] = Reward_map[75]\n",
    "Reward_map[98] = Reward_map[78]\n",
    "\n",
    "\n",
    "process = FiniteMarkovRewardProcess(Reward_map)\n",
    "reward = 0\n",
    "for transition_step in process.simulate_reward(Dists[0]): \n",
    "    print(transition_step)\n",
    "    reward += transition_step.reward\n",
    "print(reward)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96eb50e1d44aed467dc8f759cb08c32fbfa9babcf79c554e2d0e5feb04653a10"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

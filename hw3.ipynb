{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "# import graphviz\n",
    "import numpy as np\n",
    "from pprint import pprint\n",
    "from typing import (Callable, Dict, Iterable, Generic, Sequence, Tuple,\n",
    "                    Mapping, TypeVar, Set)\n",
    "from rl.policy import *\n",
    "from rl.distribution import (Categorical, Distribution, FiniteDistribution,\n",
    "                             SampledDistribution)\n",
    "\n",
    "S = TypeVar('S')\n",
    "X = TypeVar('X')\n",
    "\n",
    "class State(ABC, Generic[S]):\n",
    "    state: S\n",
    "\n",
    "    def on_non_terminal(\n",
    "        self,\n",
    "        f: Callable[[NonTerminal[S]], X],\n",
    "        default: X\n",
    "    ) -> X:\n",
    "        if isinstance(self, NonTerminal):\n",
    "            return f(self)\n",
    "        else:\n",
    "            return default\n",
    "\n",
    "    \n",
    "@dataclass(frozen=True)\n",
    "class Terminal(State[S]):\n",
    "    state: S\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class NonTerminal(State[S]):\n",
    "    state: S\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarkovProcess(ABC, Generic[S]):\n",
    "    '''A Markov process with states of type S.\n",
    "    '''\n",
    "    @abstractmethod\n",
    "    def transition(self, state: NonTerminal[S]) -> Distribution[State[S]]:\n",
    "        '''Given a state of the process, returns a distribution of\n",
    "        the next states.  Returning None means we are in a terminal state.\n",
    "        '''\n",
    "\n",
    "    def simulate(\n",
    "        self,\n",
    "        start_state_distribution: Distribution[NonTerminal[S]]\n",
    "    ) -> Iterable[State[S]]:\n",
    "        '''Run a simulation trace of this Markov process, generating the\n",
    "        states visited during the trace.\n",
    "\n",
    "        This yields the start state first, then continues yielding\n",
    "        subsequent states forever or until we hit a terminal state.\n",
    "        '''\n",
    "        state: State[S] = start_state_distribution.sample()\n",
    "        # print(start_state_distribution.sample())\n",
    "        yield state\n",
    "        # print(state)\n",
    "        while isinstance(state, NonTerminal):\n",
    "            \n",
    "            # print(self.transition(state))\n",
    "            state = self.transition(state).sample()\n",
    "            # print(state)\n",
    "            yield state\n",
    "\n",
    "    def traces(\n",
    "            self,\n",
    "            start_state_distribution: Distribution[NonTerminal[S]]\n",
    "    ) -> Iterable[Iterable[State[S]]]:\n",
    "        '''Yield simulation traces (the output of `simulate'), sampling a\n",
    "        start state from the given distribution each time.\n",
    "\n",
    "        '''\n",
    "        while True:\n",
    "            yield self.simulate(start_state_distribution)\n",
    "\n",
    "\n",
    "Transition = Mapping[NonTerminal[S], FiniteDistribution[State[S]]]\n",
    "\n",
    "\n",
    "class FiniteMarkovProcess(MarkovProcess[S]):\n",
    "    '''A Markov Process with a finite state space.\n",
    "\n",
    "    Having a finite state space lets us use tabular methods to work\n",
    "    with the process (ie dynamic programming).\n",
    "\n",
    "    '''\n",
    "\n",
    "    non_terminal_states: Sequence[NonTerminal[S]]\n",
    "    transition_map: Transition[S]\n",
    "\n",
    "\n",
    "    # def __init__(self, transition_map: Mapping[S, FiniteDistribution[S]]):\n",
    "    #     non_terminals: Set[S] = set(transition_map.keys())\n",
    "    #     self.transition_map = {\n",
    "    #         NonTerminal(s): Categorical(\n",
    "    #             {(s1 if s1 in non_terminals else s1): p\n",
    "    #              for s1, p in v.table().items()}\n",
    "    #         ) for s, v in transition_map.items()\n",
    "    #     }\n",
    "    #     self.non_terminal_states = list(self.transition_map.keys())\n",
    "\n",
    "    ###ORIGINAL ONE IS BELOW #####\n",
    "    def __init__(self, transition_map: Mapping[S, FiniteDistribution[S]]):\n",
    "        non_terminals: Set[S] = set(transition_map.keys())\n",
    "        self.transition_map = {\n",
    "            NonTerminal(s): Categorical(\n",
    "                {(NonTerminal(s1) if s1 in non_terminals else Terminal(s1)): p\n",
    "                 for s1, p in v.table().items()}\n",
    "            ) for s, v in transition_map.items()\n",
    "        }\n",
    "        self.non_terminal_states = list(self.transition_map.keys())\n",
    "        \n",
    "    def __repr__(self) -> str:\n",
    "        display = \"\"\n",
    "\n",
    "        for s, d in self.transition_map.items():\n",
    "            display += f\"From State {s.state}:\\n\"\n",
    "            for s1, p in d:\n",
    "                opt = \"Terminal \" if isinstance(s1, Terminal) else \"\"\n",
    "                display += f\"  To {opt}State {s1.state} with Probability {p:.3f}\\n\"\n",
    "\n",
    "        return display\n",
    "\n",
    "    def get_transition_matrix(self) -> np.ndarray:\n",
    "        sz = len(self.non_terminal_states)\n",
    "        mat = np.zeros((sz, sz))\n",
    "\n",
    "        for i, s1 in enumerate(self.non_terminal_states):\n",
    "            for j, s2 in enumerate(self.non_terminal_states):\n",
    "                mat[i, j] = self.transition(s1).probability(s2)\n",
    "        return mat\n",
    "\n",
    "\n",
    "    def transition(self, state: NonTerminal[S])\\\n",
    "            -> FiniteDistribution[State[S]]:\n",
    "        return self.transition_map[state]\n",
    "\n",
    "    def get_stationary_distribution(self) -> FiniteDistribution[S]:\n",
    "        eig_vals, eig_vecs = np.linalg.eig(self.get_transition_matrix().T)\n",
    "        index_of_first_unit_eig_val = np.where(\n",
    "            np.abs(eig_vals - 1) < 1e-8)[0][0]\n",
    "        eig_vec_of_unit_eig_val = np.real(\n",
    "            eig_vecs[:, index_of_first_unit_eig_val])\n",
    "        return Categorical({\n",
    "            self.non_terminal_states[i].state: ev\n",
    "            for i, ev in enumerate(eig_vec_of_unit_eig_val /\n",
    "                                   sum(eig_vec_of_unit_eig_val))\n",
    "        })\n",
    "\n",
    "    def display_stationary_distribution(self):\n",
    "        pprint({\n",
    "            s: round(p, 3)\n",
    "            for s, p in self.get_stationary_distribution()\n",
    "        })\n",
    "\n",
    "    def generate_image(self) -> graphviz.Digraph:\n",
    "        d = graphviz.Digraph()\n",
    "\n",
    "        for s in self.transition_map.keys():\n",
    "            d.node(str(s))\n",
    "\n",
    "        for s, v in self.transition_map.items():\n",
    "            for s1, p in v:\n",
    "                d.edge(str(s), str(s1), label=str(p))\n",
    "\n",
    "        return d\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from collections import defaultdict\n",
    "from dataclasses import dataclass\n",
    "from typing import (DefaultDict, Dict, Iterable, Generic, Mapping,\n",
    "                    Tuple, Sequence, TypeVar, Set)\n",
    "\n",
    "from rl.distribution import (Categorical, Distribution, FiniteDistribution)\n",
    "\n",
    "from rl.markov_process import (\n",
    "    FiniteMarkovRewardProcess, MarkovRewardProcess, StateReward, State,\n",
    "    NonTerminal, Terminal)\n",
    "from rl.policy import FinitePolicy, Policy\n",
    "\n",
    "A = TypeVar('A')\n",
    "S = TypeVar('S')\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TransitionStep(Generic[S, A]):\n",
    "    '''A single step in the simulation of an MDP, containing:\n",
    "\n",
    "    state -- the state we start from\n",
    "    action -- the action we took at that state\n",
    "    next_state -- the state we ended up in after the action\n",
    "    reward -- the instantaneous reward we got for this transition\n",
    "    '''\n",
    "    state: NonTerminal[S]\n",
    "    action: A\n",
    "    next_state: State[S]\n",
    "    reward: float\n",
    "\n",
    "    def add_return(self, γ: float, return_: float) -> ReturnStep[S, A]:\n",
    "        '''Given a γ and the return from 'next_state', this annotates the\n",
    "        transition with a return for 'state'.\n",
    "\n",
    "        '''\n",
    "        return ReturnStep(\n",
    "            self.state,\n",
    "            self.action,\n",
    "            self.next_state,\n",
    "            self.reward,\n",
    "            return_=self.reward + γ * return_\n",
    "        )\n",
    "\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class ReturnStep(TransitionStep[S, A]):\n",
    "    '''A Transition that also contains the total *return* for its starting\n",
    "    state.\n",
    "\n",
    "    '''\n",
    "    return_: float\n",
    "\n",
    "\n",
    "class MarkovDecisionProcess(ABC, Generic[S, A]):\n",
    "    def apply_policy(self, policy: Policy[S, A]) -> MarkovRewardProcess[S]:\n",
    "        mdp = self\n",
    "\n",
    "        class RewardProcess(MarkovRewardProcess[S]):\n",
    "            def transition_reward(\n",
    "                self,\n",
    "                state: NonTerminal[S]\n",
    "            ) -> Distribution[Tuple[State[S], float]]:\n",
    "                actions: Distribution[A] = policy.act(state)\n",
    "                return actions.apply(lambda a: mdp.step(state, a))\n",
    "\n",
    "        return RewardProcess()\n",
    "\n",
    "    @abstractmethod\n",
    "    def actions(self, state: NonTerminal[S]) -> Iterable[A]:\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def step(\n",
    "        self,\n",
    "        state: NonTerminal[S],\n",
    "        action: A\n",
    "    ) -> Distribution[Tuple[State[S], float]]:\n",
    "        pass\n",
    "\n",
    "    def simulate_actions(\n",
    "            self,\n",
    "            start_states: Distribution[NonTerminal[S]],\n",
    "            policy: Policy[S, A]\n",
    "    ) -> Iterable[TransitionStep[S, A]]:\n",
    "        '''Simulate this MDP with the given policy, yielding the\n",
    "        sequence of (states, action, next state, reward) 4-tuples\n",
    "        encountered in the simulation trace.\n",
    "\n",
    "        '''\n",
    "        state: State[S] = start_states.sample()\n",
    "\n",
    "        while isinstance(state, NonTerminal):\n",
    "            action_distribution = policy.act(state)\n",
    "\n",
    "            action = action_distribution.sample()\n",
    "            next_distribution = self.step(state, action)\n",
    "\n",
    "            next_state, reward = next_distribution.sample()\n",
    "            yield TransitionStep(state, action, next_state, reward)\n",
    "            state = next_state\n",
    "\n",
    "    def action_traces(\n",
    "            self,\n",
    "            start_states: Distribution[NonTerminal[S]],\n",
    "            policy: Policy[S, A]\n",
    "    ) -> Iterable[Iterable[TransitionStep[S, A]]]:\n",
    "        '''Yield an infinite number of traces as returned by\n",
    "        simulate_actions.\n",
    "\n",
    "        '''\n",
    "        while True:\n",
    "            yield self.simulate_actions(start_states, policy)\n",
    "\n",
    "\n",
    "ActionMapping = Mapping[A, StateReward[S]]\n",
    "StateActionMapping = Mapping[NonTerminal[S], ActionMapping[A, S]]\n",
    "\n",
    "\n",
    "class FiniteMarkovDecisionProcess(MarkovDecisionProcess[S, A]):\n",
    "    '''A Markov Decision Process with finite state and action spaces.\n",
    "\n",
    "    '''\n",
    "\n",
    "    mapping: StateActionMapping[S, A]\n",
    "    non_terminal_states: Sequence[NonTerminal[S]]\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        mapping: Mapping[S, Mapping[A, FiniteDistribution[Tuple[S, float]]]]\n",
    "    ):\n",
    "        non_terminals: Set[S] = set(mapping.keys())\n",
    "        self.mapping = {NonTerminal(s): {a: Categorical(\n",
    "            {(s1 if s1 in non_terminals else s1, r): p\n",
    "             for (s1, r), p in v.table().items()}\n",
    "        ) for a, v in d.items()} for s, d in mapping.items()}\n",
    "        self.non_terminal_states = list(self.mapping.keys())\n",
    "    def __repr__(self) -> str:\n",
    "        display = \"\"\n",
    "        for s, d in self.mapping.items():\n",
    "            display += f\"From State {s.state}:\\n\"\n",
    "            for a, d1 in d.items():\n",
    "                display += f\"  With Action {a}:\\n\"\n",
    "                for (s1, r), p in d1:\n",
    "                    opt = \"Terminal \" if isinstance(s1, Terminal) else \"\"\n",
    "                    display += f\"    To [{opt}State {s1.state} and \"\\\n",
    "                        + f\"Reward {r:.3f}] with Probability {p:.3f}\\n\"\n",
    "        return display\n",
    "\n",
    "    def step(self, state: NonTerminal[S], action: A) -> StateReward[S]:\n",
    "        action_map: ActionMapping[A, S] = self.mapping[state]\n",
    "        return action_map[action]\n",
    "\n",
    "    def apply_finite_policy(self, policy: FinitePolicy[S, A])\\\n",
    "            -> FiniteMarkovRewardProcess[S]:\n",
    "\n",
    "        transition_mapping: Dict[S, FiniteDistribution[Tuple[S, float]]] = {}\n",
    "\n",
    "        for state in self.mapping:\n",
    "            action_map: ActionMapping[A, S] = self.mapping[state]\n",
    "            outcomes: DefaultDict[Tuple[S, float], float]\\\n",
    "                = defaultdict(float)\n",
    "            actions = policy.act(state)\n",
    "            for action, p_action in actions:\n",
    "                for (s1, r), p in action_map[action].table().items():\n",
    "                    outcomes[(s1.state, r)] += p_action * p\n",
    "\n",
    "            transition_mapping[state.state] = Categorical(outcomes)\n",
    "\n",
    "        return FiniteMarkovRewardProcess(transition_mapping)\n",
    "\n",
    "    def actions(self, state: NonTerminal[S]) -> Iterable[A]:\n",
    "        '''All the actions allowed for the given state.\n",
    "\n",
    "        This will be empty for terminal states.\n",
    "\n",
    "        '''\n",
    "        return self.mapping[state].keys()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function: [-1.11022302e-16 -3.33333333e-01]\n",
      "Optimal Policy : {NonTerminal(state=1): 'b', NonTerminal(state=2): 'a'}\n"
     ]
    }
   ],
   "source": [
    "n = 3\n",
    "States = {}\n",
    "\n",
    "for i in range(1,n):\n",
    "    States[i] = NonTerminal(i)\n",
    "States[0] = Terminal(0)\n",
    "States[n] = Terminal(n)\n",
    "\n",
    "Dists_a: Mapping[int, float] = {}\n",
    "for i in range(1,n):\n",
    "    temp_map: Mapping[int, float] = {}\n",
    "\n",
    "    temp_map[(States[i+1],1)] = (n-i)/n\n",
    "    if i-1!=0:\n",
    "        temp_map[(States[i-1],-1)] = (i)/n # Original one\n",
    "        # temp_map[(States[i-1],1)] = (i)/n  # Just for fun\n",
    "    else:\n",
    "        temp_map[(States[i-1],-n)] = (i)/n # Original one\n",
    "        # temp_map[(States[i-1],+1)] = (i)/n # Just for fun\n",
    "    Dists_a[i] = Categorical(temp_map)\n",
    "\n",
    "\n",
    "Dists_b: Mapping[int, float] = {}\n",
    "for i in range(1,n):\n",
    "    temp_map: Mapping[int, float] = {}\n",
    "    for j in range(0,n+1):\n",
    "        if j==i:\n",
    "            continue\n",
    "        if j!=0:\n",
    "            temp_map[(States[j],j-i)] = 1/n  #Original one\n",
    "            # temp_map[(States[j],-1)] = 1/n  #For fun\n",
    "        else:\n",
    "            temp_map[(States[j],-n)] = 1/n # Original one\n",
    "            # temp_map[(States[j],-1)] = 1/n # Just for fun\n",
    "    Dists_b[i] = Categorical(temp_map)\n",
    "\n",
    "\n",
    "state_action_map: Mapping[int, float] = {}\n",
    "for i in range(1,n):\n",
    "    temp_map: Mapping={}\n",
    "    temp_map['a'] = Dists_a[i]\n",
    "    temp_map['b'] = Dists_b[i]\n",
    "    state_action_map[States[i]] = temp_map\n",
    "\n",
    "# print(state_action_map)\n",
    "mdp = FiniteMarkovDecisionProcess(state_action_map)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FinitePolicy(Policy[S, A]):\n",
    "    ''' A policy where the state and action spaces are finite.\n",
    "\n",
    "    '''\n",
    "    policy_map: Mapping[S, FiniteDistribution[A]]\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        display = \"\"\n",
    "        for s, d in self.policy_map.items():\n",
    "            display += f\"For State {s}:\\n\"\n",
    "            for a, p in d:\n",
    "                display += f\"  Do Action {a} with Probability {p:.3f}\\n\"\n",
    "        return display\n",
    "\n",
    "    def act(self, state: NonTerminal[S]) -> FiniteDistribution[A]:\n",
    "        # return self.policy_map[state] #Try this for hw3 or  hw4\n",
    "        return self.policy_map[state.state]\n",
    "\n",
    "\n",
    "class FiniteDeterministicPolicy(FinitePolicy[S, A]):\n",
    "    '''A deterministic policy where the state and action spaces are\n",
    "    finite.\n",
    "\n",
    "    '''\n",
    "    action_for: Mapping[S, A]\n",
    "\n",
    "    def __init__(self, action_for: Mapping[S, A]):\n",
    "        self.action_for = action_for\n",
    "        super().__init__(policy_map={s: Constant(a) for s, a in\n",
    "                                     self.action_for.items()})\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        display = \"\"\n",
    "        for s, a in self.action_for.items():\n",
    "            display += f\"For State {s}: Do Action {a}\\n\"\n",
    "        return display\n",
    "\n",
    "\n",
    "\n",
    "policies = []\n",
    "for i in range(2**(n-1)):\n",
    "    binary = \"{0:b}\".format(i)\n",
    "    binary = ''.join(['0' for _ in range(n-len(binary))]) + binary\n",
    "    policy: Mapping={}\n",
    "    for j in range(1,n):\n",
    "        policy[States[j]] = 'a' if binary[j] == '1' else'b'\n",
    "    policies.append(FiniteDeterministicPolicy(policy))\n",
    "\n",
    "val_fs = np.zeros((2**(n-1),n-1))\n",
    "for i in range(2**(n-1)):\n",
    "    val_f = mdp.apply_finite_policy(policies[i]).get_value_function_vec(gamma=0.5)\n",
    "    val_fs[i,:] = val_f\n",
    "c = np.max(val_fs,axis=0)\n",
    "for i in range(2**(n-1)):\n",
    "    if np.equal(c,val_fs[i,:]).all():\n",
    "        ind=i\n",
    "        print(f\"Optimal Value Function: {val_fs[ind,:]}\")\n",
    "        break\n",
    "optimal_policy:Mapping={}\n",
    "binary = \"{0:b}\".format(ind)\n",
    "binary = ''.join(['0' for _ in range(n-len(binary))]) + binary\n",
    "for j in range(1,n):\n",
    "    optimal_policy[States[j]] = 'a' if binary[j] == '1' else'b' \n",
    "print(f\"Optimal Policy : {optimal_policy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function: [ 1.5         0.5         0.         -0.33333333 -0.66666667]\n",
      "Optimal Policy : {NonTerminal(state=1): 'b', NonTerminal(state=2): 'b', NonTerminal(state=3): 'a', NonTerminal(state=4): 'a', NonTerminal(state=5): 'a'}\n"
     ]
    }
   ],
   "source": [
    "n = 6\n",
    "States = {}\n",
    "\n",
    "for i in range(1,n):\n",
    "    States[i] = NonTerminal(i)\n",
    "States[0] = Terminal(0)\n",
    "States[n] = Terminal(n)\n",
    "\n",
    "Dists_a: Mapping[int, float] = {}\n",
    "for i in range(1,n):\n",
    "    temp_map: Mapping[int, float] = {}\n",
    "\n",
    "    temp_map[(States[i+1],1)] = (n-i)/n\n",
    "    if i-1!=0:\n",
    "        temp_map[(States[i-1],-1)] = (i)/n # Original one\n",
    "        # temp_map[(States[i-1],1)] = (i)/n  # Just for fun\n",
    "    else:\n",
    "        temp_map[(States[i-1],-n)] = (i)/n # Original one\n",
    "        # temp_map[(States[i-1],+1)] = (i)/n # Just for fun\n",
    "    Dists_a[i] = Categorical(temp_map)\n",
    "\n",
    "\n",
    "Dists_b: Mapping[int, float] = {}\n",
    "for i in range(1,n):\n",
    "    temp_map: Mapping[int, float] = {}\n",
    "    for j in range(0,n+1):\n",
    "        if j==i:\n",
    "            continue\n",
    "        if j!=0:\n",
    "            temp_map[(States[j],j-i)] = 1/n  #Original one\n",
    "            # temp_map[(States[j],-1)] = 1/n  #For fun\n",
    "        else:\n",
    "            temp_map[(States[j],-n)] = 1/n # Original one\n",
    "            # temp_map[(States[j],-1)] = 1/n # Just for fun\n",
    "    Dists_b[i] = Categorical(temp_map)\n",
    "\n",
    "\n",
    "state_action_map: Mapping[int, float] = {}\n",
    "for i in range(1,n):\n",
    "    temp_map: Mapping={}\n",
    "    temp_map['a'] = Dists_a[i]\n",
    "    temp_map['b'] = Dists_b[i]\n",
    "    state_action_map[States[i]] = temp_map\n",
    "\n",
    "# print(state_action_map)\n",
    "mdp = FiniteMarkovDecisionProcess(state_action_map)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FinitePolicy(Policy[S, A]):\n",
    "    ''' A policy where the state and action spaces are finite.\n",
    "\n",
    "    '''\n",
    "    policy_map: Mapping[S, FiniteDistribution[A]]\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        display = \"\"\n",
    "        for s, d in self.policy_map.items():\n",
    "            display += f\"For State {s}:\\n\"\n",
    "            for a, p in d:\n",
    "                display += f\"  Do Action {a} with Probability {p:.3f}\\n\"\n",
    "        return display\n",
    "\n",
    "    def act(self, state: NonTerminal[S]) -> FiniteDistribution[A]:\n",
    "        # return self.policy_map[state] #Try this for hw3 or  hw4\n",
    "        return self.policy_map[state.state]\n",
    "\n",
    "\n",
    "class FiniteDeterministicPolicy(FinitePolicy[S, A]):\n",
    "    '''A deterministic policy where the state and action spaces are\n",
    "    finite.\n",
    "\n",
    "    '''\n",
    "    action_for: Mapping[S, A]\n",
    "\n",
    "    def __init__(self, action_for: Mapping[S, A]):\n",
    "        self.action_for = action_for\n",
    "        super().__init__(policy_map={s: Constant(a) for s, a in\n",
    "                                     self.action_for.items()})\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        display = \"\"\n",
    "        for s, a in self.action_for.items():\n",
    "            display += f\"For State {s}: Do Action {a}\\n\"\n",
    "        return display\n",
    "\n",
    "\n",
    "\n",
    "policies = []\n",
    "for i in range(2**(n-1)):\n",
    "    binary = \"{0:b}\".format(i)\n",
    "    binary = ''.join(['0' for _ in range(n-len(binary))]) + binary\n",
    "    policy: Mapping={}\n",
    "    for j in range(1,n):\n",
    "        policy[States[j]] = 'a' if binary[j] == '1' else'b'\n",
    "    policies.append(FiniteDeterministicPolicy(policy))\n",
    "\n",
    "val_fs = np.zeros((2**(n-1),n-1))\n",
    "for i in range(2**(n-1)):\n",
    "    val_f = mdp.apply_finite_policy(policies[i]).get_value_function_vec(gamma=0.5)\n",
    "    val_fs[i,:] = val_f\n",
    "c = np.max(val_fs,axis=0)\n",
    "for i in range(2**(n-1)):\n",
    "    if np.equal(c,val_fs[i,:]).all():\n",
    "        ind=i\n",
    "        print(f\"Optimal Value Function: {val_fs[ind,:]}\")\n",
    "        break\n",
    "optimal_policy:Mapping={}\n",
    "binary = \"{0:b}\".format(ind)\n",
    "binary = ''.join(['0' for _ in range(n-len(binary))]) + binary\n",
    "for j in range(1,n):\n",
    "    optimal_policy[States[j]] = 'a' if binary[j] == '1' else'b' \n",
    "print(f\"Optimal Policy : {optimal_policy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal Value Function: [ 3.          2.          1.          0.11111111 -0.11111111 -0.33333333\n",
      " -0.55555556 -0.77777778]\n",
      "Optimal Policy : {NonTerminal(state=1): 'b', NonTerminal(state=2): 'b', NonTerminal(state=3): 'b', NonTerminal(state=4): 'a', NonTerminal(state=5): 'a', NonTerminal(state=6): 'a', NonTerminal(state=7): 'a', NonTerminal(state=8): 'a'}\n"
     ]
    }
   ],
   "source": [
    "n = 9\n",
    "States = {}\n",
    "\n",
    "for i in range(1,n):\n",
    "    States[i] = NonTerminal(i)\n",
    "States[0] = Terminal(0)\n",
    "States[n] = Terminal(n)\n",
    "\n",
    "Dists_a: Mapping[int, float] = {}\n",
    "for i in range(1,n):\n",
    "    temp_map: Mapping[int, float] = {}\n",
    "\n",
    "    temp_map[(States[i+1],1)] = (n-i)/n\n",
    "    if i-1!=0:\n",
    "        temp_map[(States[i-1],-1)] = (i)/n # Original one\n",
    "        # temp_map[(States[i-1],1)] = (i)/n  # Just for fun\n",
    "    else:\n",
    "        temp_map[(States[i-1],-n)] = (i)/n # Original one\n",
    "        # temp_map[(States[i-1],+1)] = (i)/n # Just for fun\n",
    "    Dists_a[i] = Categorical(temp_map)\n",
    "\n",
    "\n",
    "Dists_b: Mapping[int, float] = {}\n",
    "for i in range(1,n):\n",
    "    temp_map: Mapping[int, float] = {}\n",
    "    for j in range(0,n+1):\n",
    "        if j==i:\n",
    "            continue\n",
    "        if j!=0:\n",
    "            temp_map[(States[j],j-i)] = 1/n  #Original one\n",
    "            # temp_map[(States[j],-1)] = 1/n  #For fun\n",
    "        else:\n",
    "            temp_map[(States[j],-n)] = 1/n # Original one\n",
    "            # temp_map[(States[j],-1)] = 1/n # Just for fun\n",
    "    Dists_b[i] = Categorical(temp_map)\n",
    "\n",
    "\n",
    "state_action_map: Mapping[int, float] = {}\n",
    "for i in range(1,n):\n",
    "    temp_map: Mapping={}\n",
    "    temp_map['a'] = Dists_a[i]\n",
    "    temp_map['b'] = Dists_b[i]\n",
    "    state_action_map[States[i]] = temp_map\n",
    "\n",
    "# print(state_action_map)\n",
    "mdp = FiniteMarkovDecisionProcess(state_action_map)\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class FinitePolicy(Policy[S, A]):\n",
    "    ''' A policy where the state and action spaces are finite.\n",
    "\n",
    "    '''\n",
    "    policy_map: Mapping[S, FiniteDistribution[A]]\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        display = \"\"\n",
    "        for s, d in self.policy_map.items():\n",
    "            display += f\"For State {s}:\\n\"\n",
    "            for a, p in d:\n",
    "                display += f\"  Do Action {a} with Probability {p:.3f}\\n\"\n",
    "        return display\n",
    "\n",
    "    def act(self, state: NonTerminal[S]) -> FiniteDistribution[A]:\n",
    "        # return self.policy_map[state] #Try this for hw3 or  hw4\n",
    "        return self.policy_map[state.state]\n",
    "\n",
    "\n",
    "class FiniteDeterministicPolicy(FinitePolicy[S, A]):\n",
    "    '''A deterministic policy where the state and action spaces are\n",
    "    finite.\n",
    "\n",
    "    '''\n",
    "    action_for: Mapping[S, A]\n",
    "\n",
    "    def __init__(self, action_for: Mapping[S, A]):\n",
    "        self.action_for = action_for\n",
    "        super().__init__(policy_map={s: Constant(a) for s, a in\n",
    "                                     self.action_for.items()})\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        display = \"\"\n",
    "        for s, a in self.action_for.items():\n",
    "            display += f\"For State {s}: Do Action {a}\\n\"\n",
    "        return display\n",
    "\n",
    "\n",
    "\n",
    "policies = []\n",
    "for i in range(2**(n-1)):\n",
    "    binary = \"{0:b}\".format(i)\n",
    "    binary = ''.join(['0' for _ in range(n-len(binary))]) + binary\n",
    "    policy: Mapping={}\n",
    "    for j in range(1,n):\n",
    "        policy[States[j]] = 'a' if binary[j] == '1' else'b'\n",
    "    policies.append(FiniteDeterministicPolicy(policy))\n",
    "\n",
    "val_fs = np.zeros((2**(n-1),n-1))\n",
    "for i in range(2**(n-1)):\n",
    "    val_f = mdp.apply_finite_policy(policies[i]).get_value_function_vec(gamma=0.5)\n",
    "    val_fs[i,:] = val_f\n",
    "c = np.max(val_fs,axis=0)\n",
    "for i in range(2**(n-1)):\n",
    "    if np.equal(c,val_fs[i,:]).all():\n",
    "        ind=i\n",
    "        print(f\"Optimal Value Function: {val_fs[ind,:]}\")\n",
    "        break\n",
    "optimal_policy:Mapping={}\n",
    "binary = \"{0:b}\".format(ind)\n",
    "binary = ''.join(['0' for _ in range(n-len(binary))]) + binary\n",
    "for j in range(1,n):\n",
    "    optimal_policy[States[j]] = 'a' if binary[j] == '1' else'b' \n",
    "print(f\"Optimal Policy : {optimal_policy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96eb50e1d44aed467dc8f759cb08c32fbfa9babcf79c554e2d0e5feb04653a10"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

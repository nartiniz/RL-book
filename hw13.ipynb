{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Iterable, Iterator, TypeVar, List, Sequence\n",
    "from rl.function_approx import Gradient\n",
    "import rl.markov_process as mp\n",
    "from rl.markov_decision_process import NonTerminal\n",
    "import numpy as np\n",
    "from rl.approximate_dynamic_programming import ValueFunctionApprox\n",
    "from rl.approximate_dynamic_programming import extended_vf\n",
    "from rl.markov_process import (MarkovRewardProcess, NonTerminal,\n",
    "                               FiniteMarkovRewardProcess, TransitionStep)\n",
    "from rl.function_approx import AdamGradient\n",
    "from rl.function_approx import LinearFunctionApprox\n",
    "from rl.approximate_dynamic_programming import ValueFunctionApprox\n",
    "from rl.approximate_dynamic_programming import NTStateDistribution\n",
    "from typing import Iterable, TypeVar, Callable, Iterator, Sequence\n",
    "\n",
    "from rl.markov_process import (MarkovRewardProcess, NonTerminal,\n",
    "                               FiniteMarkovRewardProcess, TransitionStep)\n",
    "import itertools\n",
    "from rl.chapter3.simple_inventory_mdp_cap import *\n",
    "import rl.iterate as iterate\n",
    "from rl.returns import returns\n",
    "import rl.monte_carlo as mc\n",
    "from rl.function_approx import learning_rate_schedule\n",
    "import rl.td as td\n",
    "import rl.td_lambda as td_lambda\n",
    "from rl.approximate_dynamic_programming import ValueFunctionApprox\n",
    "from rl.approximate_dynamic_programming import NTStateDistribution\n",
    "import numpy as np\n",
    "from rl.chapter2.simple_inventory_mrp import SimpleInventoryMRPFinite\n",
    "from rl.chapter2.simple_inventory_mrp import InventoryState\n",
    "from itertools import islice\n",
    "from rl.distribution import Choose\n",
    "\n",
    "\n",
    "from typing import TypeVar, Callable, Iterator, Sequence, Tuple, Mapping\n",
    "from rl.function_approx import Tabular\n",
    "from rl.distribution import Choose\n",
    "from rl.markov_process import NonTerminal\n",
    "from rl.markov_decision_process import (\n",
    "    MarkovDecisionProcess, FiniteMarkovDecisionProcess,\n",
    "    FiniteMarkovRewardProcess)\n",
    "from rl.policy import FiniteDeterministicPolicy, FinitePolicy\n",
    "from rl.approximate_dynamic_programming import QValueFunctionApprox\n",
    "from rl.approximate_dynamic_programming import NTStateDistribution\n",
    "import itertools\n",
    "import rl.iterate as iterate\n",
    "from rl.returns import returns\n",
    "import rl.monte_carlo as mc\n",
    "import rl.td as td\n",
    "from rl.function_approx import learning_rate_schedule\n",
    "from rl.dynamic_programming import V, value_iteration_result\n",
    "from math import sqrt\n",
    "from pprint import pprint\n",
    "S = TypeVar('S')\n",
    "A = TypeVar('A')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0\n",
      "iteration : 500\n",
      "iteration : 1000\n",
      "iteration : 1500\n",
      "iteration : 2000\n",
      "iteration : 2500\n",
      "iteration : 3000\n",
      "iteration : 3500\n",
      "iteration : 4000\n",
      "iteration : 4500\n",
      "iteration : 5000\n",
      "iteration : 5500\n",
      "iteration : 6000\n",
      "iteration : 6500\n",
      "iteration : 7000\n",
      "iteration : 7500\n",
      "iteration : 8000\n",
      "iteration : 8500\n",
      "iteration : 9000\n",
      "iteration : 9500\n",
      "iteration : 10000\n",
      "iteration : 10500\n",
      "iteration : 11000\n",
      "iteration : 11500\n",
      "iteration : 12000\n",
      "iteration : 12500\n",
      "iteration : 13000\n",
      "iteration : 13500\n",
      "iteration : 14000\n",
      "iteration : 14500\n",
      "iteration : 15000\n",
      "iteration : 15500\n",
      "iteration : 16000\n",
      "iteration : 16500\n",
      "iteration : 17000\n",
      "iteration : 17500\n",
      "iteration : 18000\n",
      "iteration : 18500\n",
      "iteration : 19000\n",
      "iteration : 19500\n",
      "iteration : 20000\n",
      "iteration : 20500\n",
      "iteration : 21000\n",
      "iteration : 21500\n",
      "iteration : 22000\n",
      "iteration : 22500\n",
      "iteration : 23000\n",
      "iteration : 23500\n",
      "iteration : 24000\n",
      "iteration : 24500\n",
      "iteration : 25000\n",
      "iteration : 25500\n",
      "iteration : 26000\n",
      "iteration : 26500\n",
      "iteration : 27000\n",
      "iteration : 27500\n",
      "iteration : 28000\n",
      "iteration : 28500\n",
      "iteration : 29000\n",
      "iteration : 29500\n",
      "iteration : 30000\n",
      "iteration : 30500\n",
      "iteration : 31000\n",
      "iteration : 31500\n",
      "iteration : 32000\n",
      "iteration : 32500\n",
      "iteration : 33000\n",
      "iteration : 33500\n",
      "iteration : 34000\n",
      "iteration : 34500\n",
      "iteration : 35000\n",
      "iteration : 35500\n",
      "iteration : 36000\n",
      "iteration : 36500\n",
      "iteration : 37000\n",
      "iteration : 37500\n",
      "iteration : 38000\n",
      "iteration : 38500\n",
      "iteration : 39000\n",
      "iteration : 39500\n",
      "iteration : 40000\n",
      "iteration : 40500\n",
      "iteration : 41000\n",
      "iteration : 41500\n",
      "iteration : 42000\n",
      "iteration : 42500\n",
      "iteration : 43000\n",
      "iteration : 43500\n",
      "iteration : 44000\n",
      "iteration : 44500\n",
      "iteration : 45000\n",
      "iteration : 45500\n",
      "iteration : 46000\n",
      "iteration : 46500\n",
      "iteration : 47000\n",
      "iteration : 47500\n",
      "iteration : 48000\n",
      "iteration : 48500\n",
      "iteration : 49000\n",
      "iteration : 49500\n",
      "iteration : 50000\n",
      "iteration : 50500\n",
      "iteration : 51000\n",
      "iteration : 51500\n",
      "iteration : 52000\n",
      "iteration : 52500\n",
      "iteration : 53000\n",
      "iteration : 53500\n",
      "iteration : 54000\n",
      "iteration : 54500\n",
      "iteration : 55000\n",
      "iteration : 55500\n",
      "iteration : 56000\n",
      "iteration : 56500\n",
      "iteration : 57000\n",
      "iteration : 57500\n",
      "iteration : 58000\n",
      "iteration : 58500\n",
      "iteration : 59000\n",
      "iteration : 59500\n",
      "iteration : 60000\n",
      "iteration : 60500\n",
      "iteration : 61000\n",
      "iteration : 61500\n",
      "iteration : 62000\n",
      "iteration : 62500\n",
      "iteration : 63000\n",
      "iteration : 63500\n",
      "iteration : 64000\n",
      "iteration : 64500\n",
      "iteration : 65000\n",
      "iteration : 65500\n",
      "iteration : 66000\n",
      "iteration : 66500\n",
      "iteration : 67000\n",
      "iteration : 67500\n",
      "iteration : 68000\n",
      "iteration : 68500\n",
      "iteration : 69000\n",
      "iteration : 69500\n",
      "iteration : 70000\n",
      "iteration : 70500\n",
      "iteration : 71000\n",
      "iteration : 71500\n",
      "iteration : 72000\n",
      "iteration : 72500\n",
      "iteration : 73000\n",
      "iteration : 73500\n",
      "iteration : 74000\n",
      "iteration : 74500\n",
      "iteration : 75000\n",
      "iteration : 75500\n",
      "iteration : 76000\n",
      "iteration : 76500\n",
      "iteration : 77000\n",
      "iteration : 77500\n",
      "iteration : 78000\n",
      "iteration : 78500\n",
      "iteration : 79000\n",
      "iteration : 79500\n",
      "iteration : 80000\n",
      "iteration : 80500\n",
      "iteration : 81000\n",
      "iteration : 81500\n",
      "iteration : 82000\n",
      "iteration : 82500\n",
      "iteration : 83000\n",
      "iteration : 83500\n",
      "iteration : 84000\n",
      "iteration : 84500\n",
      "iteration : 85000\n",
      "iteration : 85500\n",
      "iteration : 86000\n",
      "iteration : 86500\n",
      "iteration : 87000\n",
      "iteration : 87500\n",
      "iteration : 88000\n",
      "iteration : 88500\n",
      "iteration : 89000\n",
      "iteration : 89500\n",
      "iteration : 90000\n",
      "iteration : 90500\n",
      "iteration : 91000\n",
      "iteration : 91500\n",
      "iteration : 92000\n",
      "iteration : 92500\n",
      "iteration : 93000\n",
      "iteration : 93500\n",
      "iteration : 94000\n",
      "iteration : 94500\n",
      "iteration : 95000\n",
      "iteration : 95500\n",
      "iteration : 96000\n",
      "iteration : 96500\n",
      "iteration : 97000\n",
      "iteration : 97500\n",
      "iteration : 98000\n",
      "iteration : 98500\n",
      "iteration : 99000\n",
      "iteration : 99500\n",
      "sarsa QVF [-26.78551271 -21.0866844  -22.07267694 -16.61986251 -13.84235257\n",
      " -14.90550563 -19.49859874 -15.21720705 -15.2674826  -15.37380032]\n",
      "Sarsa Value Function : [-21.086684396117803, -13.842352573382744, -14.90550563219221, -15.217207054521534, -15.267482601248952, -15.373800322527648]\n",
      "True VF : [-20.69406782 -13.25880178 -13.36758477 -14.25880178 -14.36758477\n",
      " -15.36758477]\n"
     ]
    }
   ],
   "source": [
    "###### SARSA  ######\n",
    "from operator import itemgetter\n",
    "import itertools\n",
    "from typing import Callable, Iterable, Iterator, TypeVar, Set, Sequence, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from rl.approximate_dynamic_programming import ValueFunctionApprox, \\\n",
    "    QValueFunctionApprox, NTStateDistribution, extended_vf\n",
    "from rl.distribution import Categorical\n",
    "from rl.function_approx import LinearFunctionApprox, Weights\n",
    "import rl.iterate as iterate\n",
    "import rl.markov_process as mp\n",
    "from rl.markov_decision_process import MarkovDecisionProcess\n",
    "from rl.markov_decision_process import TransitionStep, NonTerminal\n",
    "from rl.monte_carlo import greedy_policy_from_qvf\n",
    "from rl.policy import Policy, DeterministicPolicy\n",
    "from rl.experience_replay import ExperienceReplayMemory\n",
    "def epsilon_greedy_action(\n",
    "    q: QValueFunctionApprox[S, A],\n",
    "    nt_state: NonTerminal[S],\n",
    "    actions: Set[A],\n",
    "    ϵ: float\n",
    ") -> A:\n",
    "    '''\n",
    "    given a non-terminal state, a Q-Value Function (in the form of a\n",
    "    FunctionApprox: (state, action) -> Value, and epislon, return\n",
    "    an action sampled from the probability distribution implied by an\n",
    "    epsilon-greedy policy that is derived from the Q-Value Function.\n",
    "    '''\n",
    "    greedy_action: A = max(\n",
    "        ((a, q((nt_state, a))) for a in actions),\n",
    "        key=itemgetter(1)\n",
    "    )[0]\n",
    "    return Categorical(\n",
    "        {a: ϵ / len(actions) +\n",
    "         (1 - ϵ if a == greedy_action else 0.) for a in actions}\n",
    "    ).sample()\n",
    "def glie_sarsa(\n",
    "    mdp: FiniteMarkovDecisionProcess[S, A],\n",
    "    states: NTStateDistribution[S],\n",
    "    approx_0: QValueFunctionApprox[S, A],\n",
    "    γ: float,\n",
    "    ϵ_as_func_of_episodes: Callable[[int], float],\n",
    "    max_episode_length: int\n",
    ") -> Iterator[QValueFunctionApprox[S, A]]:\n",
    "    q: QValueFunctionApprox[S, A] = approx_0\n",
    "    yield q\n",
    "    num_episodes: int = 0\n",
    "    while True:\n",
    "        num_episodes += 1\n",
    "        ϵ: float = ϵ_as_func_of_episodes(num_episodes)\n",
    "        state: NonTerminal[S] = states.sample()\n",
    "        action: A = epsilon_greedy_action(\n",
    "            q=q,\n",
    "            nt_state=state,\n",
    "            actions=set(mdp.actions(state)),\n",
    "            ϵ=ϵ\n",
    "        )\n",
    "        steps: int = 0\n",
    "        while isinstance(state, NonTerminal) and steps < max_episode_length:\n",
    "            next_state, reward = mdp.step(state, action).sample()\n",
    "            if isinstance(next_state, NonTerminal):\n",
    "                next_action: A = epsilon_greedy_action(\n",
    "                    q=q,\n",
    "                    nt_state=next_state,\n",
    "                    actions=set(mdp.actions(next_state)),\n",
    "                    ϵ=ϵ\n",
    "                )\n",
    "                q = q.update([(\n",
    "                    (state, action),\n",
    "                    reward + γ * q((next_state, next_action))\n",
    "                )])\n",
    "                action = next_action\n",
    "            else:\n",
    "                q = q.update([((state, action), reward)])\n",
    "            yield q\n",
    "            steps += 1\n",
    "            state = next_state\n",
    "\n",
    "def glie_sarsa_learning_rate(\n",
    "    mdp: MarkovDecisionProcess[S, A],\n",
    "    start_state_distribution: NTStateDistribution[S],\n",
    "    initial_func_approx: QValueFunctionApprox[S, A],\n",
    "    gamma: float,\n",
    "    epsilon_as_func_of_episodes: Callable[[int], float],\n",
    "    max_episode_length: int\n",
    ") -> Iterator[QValueFunctionApprox[S, A]]:\n",
    "    return glie_sarsa(\n",
    "        mdp=mdp,\n",
    "        states=start_state_distribution,\n",
    "        approx_0=initial_func_approx,\n",
    "        γ=gamma,\n",
    "        ϵ_as_func_of_episodes=epsilon_as_func_of_episodes,\n",
    "        max_episode_length=max_episode_length\n",
    "    )\n",
    "\n",
    "capacity: int = 2\n",
    "poisson_lambda: float = 1.0\n",
    "holding_cost: float = 1.0\n",
    "stockout_cost: float = 10.0\n",
    "\n",
    "gamma: float = 0.9\n",
    "gamma: float = 0.8  ### I CHOOSE THIS GAMMA FOR FAST CONVERGENCE \n",
    "## CLASSICAL SIMPLE INVENTORY\n",
    "si_mrp = SimpleInventoryMRPFinite(\n",
    "    capacity=capacity,\n",
    "    poisson_lambda=poisson_lambda,\n",
    "    holding_cost=holding_cost,\n",
    "    stockout_cost=stockout_cost\n",
    ")\n",
    "\n",
    "si_mdp=SimpleInventoryMDPCap(\n",
    "    capacity=capacity,\n",
    "    poisson_lambda=poisson_lambda,\n",
    "    holding_cost=holding_cost,\n",
    "    stockout_cost=stockout_cost\n",
    ")\n",
    "all_states = si_mdp.non_terminal_states\n",
    "true_vf = si_mrp.get_value_function_vec(gamma=gamma)\n",
    "\n",
    "\n",
    "mc_episode_length_tol: float = 1e-6\n",
    "num_episodes = 10000\n",
    "\n",
    "td_episode_length: int = 100\n",
    "initial_learning_rate: float = 0.03\n",
    "half_life: float = 1000.0\n",
    "exponent: float = 0.5\n",
    "\n",
    "#### FUNCTION APPROXIMATION WITH STATE INDICATOR FEATURES  ####\n",
    "ag = AdamGradient(\n",
    "    learning_rate=0.05,\n",
    "    decay1=0.9,\n",
    "    decay2=0.999\n",
    ")\n",
    "\n",
    "\n",
    "### This is for tabular case, but we can generalize by changing our functions. \n",
    "### For example we can take x[0].state.on_hand as a parameter, or even lets say\n",
    "### we can take x[0].state.on_hand^2 as a feature.\n",
    "ffs=[]\n",
    "for s in si_mdp.mapping.keys():\n",
    "    for a in si_mdp.mapping[s].keys():\n",
    "        ffs.append(lambda x,s=s,a=a:float(x[0].state==s.state and x[1]==a))\n",
    "\n",
    "lfa = LinearFunctionApprox.create(\n",
    "        feature_functions=ffs,\n",
    "        adam_gradient=ag,\n",
    "        direct_solve=False\n",
    ")\n",
    "episode_length=1000\n",
    "sarsa: Iterable[QValueFunctionApprox[InventoryState,float]] = \\\n",
    "    glie_sarsa_learning_rate(\n",
    "        mdp=si_mdp,\n",
    "        start_state_distribution=Choose(all_states),\n",
    "        gamma=gamma,\n",
    "        initial_func_approx=lfa,\n",
    "        max_episode_length=1000,\n",
    "        epsilon_as_func_of_episodes=lambda x: 0.1)\n",
    "\n",
    "import math\n",
    "sarsa_rmse_vals=[]\n",
    "sarsa_episodes: int = 100000\n",
    "for i, sarsa_vf in enumerate(islice(sarsa, sarsa_episodes)):\n",
    "    if i%2000==0:\n",
    "        print(f\"iteration : {i}\")\n",
    "print(f\"sarsa QVF {sarsa_vf.weights.weights}\")\n",
    "vf_list = []\n",
    "for s in si_mdp.mapping.keys():\n",
    "    maxi = -math.inf\n",
    "    for a in si_mdp.mapping[s].keys():\n",
    "        if maxi<= sarsa_vf.evaluate([(s,a)])[0]:\n",
    "            maxi=sarsa_vf.evaluate([(s,a)])[0]\n",
    "    vf_list.append(maxi)\n",
    "print(f\"Sarsa Value Function : {vf_list}\")\n",
    "print(f\"True VF : {true_vf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 0\n",
      "iteration : 2000\n",
      "iteration : 4000\n",
      "iteration : 6000\n",
      "iteration : 8000\n",
      "iteration : 10000\n",
      "iteration : 12000\n",
      "iteration : 14000\n",
      "iteration : 16000\n",
      "iteration : 18000\n",
      "iteration : 20000\n",
      "iteration : 22000\n",
      "iteration : 24000\n",
      "iteration : 26000\n",
      "iteration : 28000\n",
      "iteration : 30000\n",
      "iteration : 32000\n",
      "iteration : 34000\n",
      "iteration : 36000\n",
      "iteration : 38000\n",
      "iteration : 40000\n",
      "iteration : 42000\n",
      "iteration : 44000\n",
      "iteration : 46000\n",
      "iteration : 48000\n",
      "qlearning QVF [-26.58903029 -21.10830166 -20.45944982 -18.16895884 -13.32752454\n",
      " -12.8264834  -16.83930414 -14.60149306 -13.6732792  -14.6565989 ]\n",
      "Q Learning Value Function : [-20.459449822593385, -13.327524537045528, -12.826483400169202, -14.60149306134891, -13.673279202942279, -14.656598902360578]\n",
      "True VF : [-20.69406782 -13.25880178 -13.36758477 -14.25880178 -14.36758477\n",
      " -15.36758477]\n"
     ]
    }
   ],
   "source": [
    "###### Q-LEARNING #####\n",
    "def q_learning_learning_rate(\n",
    "    mdp: MarkovDecisionProcess[S, A],\n",
    "    start_state_distribution: NTStateDistribution[S],\n",
    "    initial_func_approx: QValueFunctionApprox[S, A],\n",
    "    gamma: float,\n",
    "    epsilon: float,\n",
    "    max_episode_length: int\n",
    ") -> Iterator[QValueFunctionApprox[S, A]]:\n",
    "    return td.q_learning(\n",
    "        mdp=mdp,\n",
    "        policy_from_q=lambda f, m: mc.epsilon_greedy_policy(\n",
    "            q=f,\n",
    "            mdp=m,\n",
    "            ϵ=epsilon\n",
    "        ),\n",
    "        states=start_state_distribution,\n",
    "        approx_0=initial_func_approx,\n",
    "        γ=gamma,\n",
    "        max_episode_length=max_episode_length\n",
    "    )\n",
    "capacity: int = 2\n",
    "poisson_lambda: float = 1.0\n",
    "holding_cost: float = 1.0\n",
    "stockout_cost: float = 10.0\n",
    "\n",
    "gamma: float = 0.9\n",
    "gamma: float = 0.8  ### I CHOOSE THIS GAMMA FOR FAST CONVERGENCE \n",
    "## CLASSICAL SIMPLE INVENTORY\n",
    "si_mrp = SimpleInventoryMRPFinite(\n",
    "    capacity=capacity,\n",
    "    poisson_lambda=poisson_lambda,\n",
    "    holding_cost=holding_cost,\n",
    "    stockout_cost=stockout_cost\n",
    ")\n",
    "\n",
    "si_mdp=SimpleInventoryMDPCap(\n",
    "    capacity=capacity,\n",
    "    poisson_lambda=poisson_lambda,\n",
    "    holding_cost=holding_cost,\n",
    "    stockout_cost=stockout_cost\n",
    ")\n",
    "all_states = si_mdp.non_terminal_states\n",
    "true_vf = si_mrp.get_value_function_vec(gamma=gamma)\n",
    "\n",
    "\n",
    "mc_episode_length_tol: float = 1e-6\n",
    "num_episodes = 10000\n",
    "\n",
    "td_episode_length: int = 100\n",
    "initial_learning_rate: float = 0.03\n",
    "half_life: float = 1000.0\n",
    "exponent: float = 0.5\n",
    "\n",
    "#### FUNCTION APPROXIMATION WITH STATE INDICATOR FEATURES  ####\n",
    "ag = AdamGradient(\n",
    "    learning_rate=0.05,\n",
    "    decay1=0.9,\n",
    "    decay2=0.999\n",
    ")\n",
    "\n",
    "### This is for tabular case, but we can generalize by changing our functions. \n",
    "### For example we can take x[0].state.on_hand as a parameter, or even lets say\n",
    "### we can take x[0].state.on_hand^2 as a feature.\n",
    "ffs=[]\n",
    "for s in si_mdp.mapping.keys():\n",
    "    for a in si_mdp.mapping[s].keys():\n",
    "        ffs.append(lambda x,s=s,a=a:float(x[0].state==s.state and x[1]==a))\n",
    "        \n",
    "lfa = LinearFunctionApprox.create(\n",
    "        feature_functions=ffs,\n",
    "        adam_gradient=ag,\n",
    "        direct_solve=False\n",
    ")\n",
    "episode_length=1000\n",
    "q_lear: Iterable[QValueFunctionApprox[InventoryState,float]] = \\\n",
    "    q_learning_learning_rate(\n",
    "        mdp=si_mdp,\n",
    "        start_state_distribution=Choose(all_states),\n",
    "        gamma=gamma,\n",
    "        initial_func_approx=lfa,\n",
    "        max_episode_length=1000,\n",
    "        epsilon=0.1,\n",
    "    )\n",
    "\n",
    "sarsa_rmse_vals=[]\n",
    "q_lear_episodes: int = 50000\n",
    "for i, qlear_vf in enumerate(islice(q_lear, q_lear_episodes)):\n",
    "    if i%2000==0:\n",
    "        print(f\"iteration : {i}\")\n",
    "print(f\"qlearning QVF {qlear_vf.weights.weights}\")\n",
    "\n",
    "vf_list = []\n",
    "for s in si_mdp.mapping.keys():\n",
    "    maxi = -math.inf\n",
    "    for a in si_mdp.mapping[s].keys():\n",
    "        if maxi<= qlear_vf.evaluate([(s,a)])[0]:\n",
    "            maxi=qlear_vf.evaluate([(s,a)])[0]\n",
    "    vf_list.append(maxi)\n",
    "print(f\"Q Learning Value Function : {vf_list}\")\n",
    "\n",
    "print(f\"True VF : {true_vf}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GLIE MC Optimal Value Function with 10000 episodes\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -35.085719335513666,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.71389758645516,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.558080366141244,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -29.06629022397233,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.46907723115718,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.36796015491581}\n",
      "GLIE MC Optimal Policy with 10000 episodes\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n",
      "True Optimal Value Function\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.894939969029906,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.661044419037374,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.9919842788034,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.66104441903738,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.9919842788034,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -29.9919842788034}\n",
      "True Optimal Policy\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n",
      "GLIE SARSA Optimal Value Function with 1000000 updates\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -35.135965189652694,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.91629207236458,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -28.59937029491664,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.992472499114857,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -29.503785183963966,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.175624723087704}\n",
      "GLIE SARSA Optimal Policy with 1000000 updates\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n",
      "True Optimal Value Function\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.894939969029906,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.661044419037374,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.9919842788034,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.66104441903738,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.9919842788034,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -29.9919842788034}\n",
      "True Optimal Policy\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n",
      "Q-Learning ptimal Value Function with 1000000 updates\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.76013123666477,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.517058311507025,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.81748717094445,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.539132246734287,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.881291643190355,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -30.036040337719157}\n",
      "Q-Learning Optimal Policy with 1000000 updates\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n",
      "True Optimal Value Function\n",
      "{NonTerminal(state=InventoryState(on_hand=0, on_order=0)): -34.894939969029906,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=1)): -27.661044419037374,\n",
      " NonTerminal(state=InventoryState(on_hand=0, on_order=2)): -27.9919842788034,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=0)): -28.66104441903738,\n",
      " NonTerminal(state=InventoryState(on_hand=1, on_order=1)): -28.9919842788034,\n",
      " NonTerminal(state=InventoryState(on_hand=2, on_order=0)): -29.9919842788034}\n",
      "True Optimal Policy\n",
      "For State InventoryState(on_hand=0, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=1): Do Action 1\n",
      "For State InventoryState(on_hand=0, on_order=2): Do Action 0\n",
      "For State InventoryState(on_hand=1, on_order=0): Do Action 1\n",
      "For State InventoryState(on_hand=1, on_order=1): Do Action 0\n",
      "For State InventoryState(on_hand=2, on_order=0): Do Action 0\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1100x700 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ############## IGNORE THIS PART  ##############\n",
    "# from typing import Tuple, Callable, Sequence\n",
    "# from rl.chapter11.control_utils import glie_mc_finite_learning_rate_correctness\n",
    "# from rl.chapter11.control_utils import \\\n",
    "#     q_learning_finite_learning_rate_correctness\n",
    "# from rl.chapter11.control_utils import \\\n",
    "#     glie_sarsa_finite_learning_rate_correctness\n",
    "# from rl.chapter11.control_utils import compare_mc_sarsa_ql\n",
    "# from rl.chapter3.simple_inventory_mdp_cap import SimpleInventoryMDPCap\n",
    "\n",
    "\n",
    "# capacity: int = 2\n",
    "# poisson_lambda: float = 1.0\n",
    "# holding_cost: float = 1.0\n",
    "# stockout_cost: float = 10.0\n",
    "\n",
    "# si_mdp: SimpleInventoryMDPCap = SimpleInventoryMDPCap(\n",
    "#     capacity=capacity,\n",
    "#     poisson_lambda=poisson_lambda,\n",
    "#     holding_cost=holding_cost,\n",
    "#     stockout_cost=stockout_cost\n",
    "# )\n",
    "\n",
    "# gamma: float = 0.9\n",
    "# mc_episode_length_tol: float = 1e-5\n",
    "# num_episodes = 10000\n",
    "\n",
    "# epsilon_as_func_of_episodes: Callable[[int], float] = lambda k: k ** -0.5\n",
    "# q_learning_epsilon: float = 0.2\n",
    "\n",
    "# td_episode_length: int = 100\n",
    "# initial_learning_rate: float = 0.1\n",
    "# half_life: float = 10000.0\n",
    "# exponent: float = 1.0\n",
    "\n",
    "# glie_mc_finite_learning_rate_correctness(\n",
    "#     fmdp=si_mdp,\n",
    "#     initial_learning_rate=initial_learning_rate,\n",
    "#     half_life=half_life,\n",
    "#     exponent=exponent,\n",
    "#     gamma=gamma,\n",
    "#     epsilon_as_func_of_episodes=epsilon_as_func_of_episodes,\n",
    "#     episode_length_tolerance=mc_episode_length_tol,\n",
    "#     num_episodes=num_episodes\n",
    "# )\n",
    "\n",
    "# glie_sarsa_finite_learning_rate_correctness(\n",
    "#     fmdp=si_mdp,\n",
    "#     initial_learning_rate=initial_learning_rate,\n",
    "#     half_life=half_life,\n",
    "#     exponent=exponent,\n",
    "#     gamma=gamma,\n",
    "#     epsilon_as_func_of_episodes=epsilon_as_func_of_episodes,\n",
    "#     max_episode_length=td_episode_length,\n",
    "#     num_updates=num_episodes * td_episode_length\n",
    "# )\n",
    "\n",
    "# q_learning_finite_learning_rate_correctness(\n",
    "#     fmdp=si_mdp,\n",
    "#     initial_learning_rate=initial_learning_rate,\n",
    "#     half_life=half_life,\n",
    "#     exponent=exponent,\n",
    "#     gamma=gamma,\n",
    "#     epsilon=q_learning_epsilon,\n",
    "#     max_episode_length=td_episode_length,\n",
    "#     num_updates=num_episodes * td_episode_length\n",
    "# )\n",
    "\n",
    "# num_episodes = 500\n",
    "# plot_batch: int = 10\n",
    "# plot_start: int = 0\n",
    "# learning_rates: Sequence[Tuple[float, float, float]] = \\\n",
    "#     [(0.05, 1000000, 0.5)]\n",
    "\n",
    "# compare_mc_sarsa_ql(\n",
    "#     fmdp=si_mdp,\n",
    "#     method_mask=[True, True, False],\n",
    "#     learning_rates=learning_rates,\n",
    "#     gamma=gamma,\n",
    "#     epsilon_as_func_of_episodes=epsilon_as_func_of_episodes,\n",
    "#     q_learning_epsilon=q_learning_epsilon,\n",
    "#     mc_episode_length_tol=mc_episode_length_tol,\n",
    "#     num_episodes=num_episodes,\n",
    "#     plot_batch=plot_batch,\n",
    "#     plot_start=plot_start\n",
    "# )"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96eb50e1d44aed467dc8f759cb08c32fbfa9babcf79c554e2d0e5feb04653a10"
  },
  "kernelspec": {
   "display_name": "Python 3.7.3 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
